{
  "hash": "54f940847982a70059350013b10666f2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NAO Cost Estimate MVP -- Adding noise\"\nauthor: \"Dan Rice\"\ndate: 2024-03-04\nformat:\n  html:\n    code-fold: false\n    toc: true\njupyter: python3\nfilters:\n    - black-formatter\n---\n\n\n## Background\n\n[Previously](../2024-02-02_CostEstimateMVP/index.qmd), we have worked out a deterministic model of the sequencing depth required to detect a novel pandemic virus by the time it reaches a fixed cumulative incidence (number of people ever infected).\nHowever, in the real world, there are a number of sources of noise that will affect our ability to detect a virus.\nIn this post, we ask the question:\n**For a given level of sequencing, what is the cumulative incidence by which we have an x% chance of detecting a virus?**\nWe can use this result to modify our deterministic estimates of the level of sequencing required to detect by a given cumulative incidence.\n\n## Model\n\nIt is useful to categorize noise sources by how they behave as various parameters like the sequencing depth change.\nFor example, three types of noise that are relevant to our problem are:\n\n1. Noise whose coefficient of variation decreases as the sequencing depth increases.\n   This includes poisson counting noise in the number of reads mapping to a sequence,\n   due to finite sampling effects.\n   Variation in the number of reads per sample likely takes this form as well.\n2. Noise whose coefficient of variation goes to a constant value as the sequencing depth increases.\n   For example, random variation in the efficiency of the target will contribute this type of noise.\n   Also, the relative abundances in a sequencing library depend on biases in enrichment efficiency.\n   If there is a class of abundant sequences that are efficiently enriched by our lab protocol, random variation in the abundance of that class of sequences will generate noise in the counts of all the other sequences that is only weakly dependent on the total read depth.\n3. Noise that depends on the number of people contributing to a sample.\n   For example, in the limit where each sample is taken from a single person,\n   the noise in the counts of reads mapping to the pandemic virus will be dominated by whether that single person is infected or not.\n\nIn the following, we consider noise classes 1 and 2.\nWe neglect 3 for the moment because in well-mixed municipal wastewater samples, we expect this to be a small effect.\nHowever, similar analysis to that presented here could be applied in that case as well.\n(Update: see Appendix for update with a treatment of 3).\n\nWe will consider a sequence of samples indexed by $i$, where the random variable $Y_i$ represents the number of reads corresponding to the pandemic virus in sample $i$.\nWe model $Y_i$ as independent draws from a Poisson mixture distribution:\n$$\nY_i \\sim \\text{Poisson}(X_i),\n$$\nwhere $X_i$ is a latent variable that represents excess noise not accounted for by the Poisson model.\nTo connect this model to our previous deterministic model, we set the mean of $X_i$ to the number of reads in the deterministic model:\n\n$$\nE[X_i] = \\mu_i = \\frac{n b}{N} e^{r(t_0 + i \\delta t)}\n$$\n\nwhere:\n\n- $n$ is the sequencing depth\n- $b$ is the P2RA factor\n- $N$ is the population size\n- $r$ is the growth rate of the virus\n- $t_0$ is the time of the first sample after the start of the pandemic\n- $\\delta t$ is the time between samples.\n\nNote that for simplicity this is assuming instantaneous grab sampling, which is a good approximation to 24-hr composite sampling.\n\nRecall that in our detection model, we declare a virus to be detected when the cumulative number of reads matching the virus cross a threshold.\nThus, to calculate the probability of detection, we need to calculate the probability that the cumulative number of reads ($\\sum_{j=0}^{i} Y_j$) is greater than the threshold value $\\hat{K}$.\nWe will proceed in two steps:\n\n1. Calculate the cumulant generating function (CGF) of the random variable $Y = \\sum_j Y_j$.\n   It is convenient to work with the CGF because the CGF of a sum of independent random variables is the sum of their individual CGFs.\n2. Approximate the cumulative distribution function (CDF) of $Y$ from a finite set of cumulants using the [Cornish-Fisher expansion](https://en.wikipedia.org/wiki/Cornish%E2%80%93Fisher_expansion).\n   In this notebook, we will explore under what conditions we can truncate the Cornish-Fisher expansion at a certain number of terms.\n\n## Cumulant generating function of the cumulative read count, $Y$\n\nThe cumulant generating function $K_Y$ of random variable $Y$ is given by the log of its moment generating function:\n\n$$\nK_Y(z) = \\log \\mathbb{E}[e^{zY}].\n$$\n\nIf $Y_i$ is Poisson distributed with random mean $X_i$,\n\n$$\n\\begin{align}\nK_{Y_i}(z) & = \\log \\mathbb{E}\\left[\\mathbb{E}[e^{zY_{i}} | X_i]\\right] \\\\\n       & = \\log \\mathbb{E}\\left[\\exp \\left\\{ X_i (e^{z} - 1) \\right\\} \\right] \\\\\n       & = K_{X_i} \\left(e^{z} - 1\\right),\n\\end{align}\n$$\nwhere the second line uses the moment-generating fuction of a Poisson random variable,\nand $K_{X_i}$ is the CGF of $X_i$.\n\nIf we assume that the $Y_i$ are independent of one another, then we can add their CGFs to get the CGF of the cumulative read count:\n$$\n\\begin{align}\nK_Y(z) & = K_{\\sum_i Y_i}(z) \\\\\n       & = \\sum_i K_{Y_i}(z) \\\\\n       & = \\sum_i K_{X_i}(e^z - 1) \\\\\n       & = K_{X}(e^z - 1),\n\\end{align}\n$$\nwhere we define $X \\equiv \\sum_i X_i$.\n\nThe last equation tells us how to combine the cumulants of $X$ to get the cumulants of $Y$.\nLet the cumulants of $Y$ be denoted $\\kappa_1, \\kappa_2, \\ldots$ and the cumulants of $X$ by $\\chi_1, \\chi_2, \\ldots$.\nExpanding the CGF gives:\n$$\n\\begin{align}\nK_Y(z) & = K_X(e^z - 1) \\\\\n       & = \\chi_1 (e^z - 1) + \\chi_2 \\frac{{(e^z-1)}^2}{2} + \\chi_3 \\frac{{(e^z-1)}^3}{3!} + \\cdots \\\\\n       & = \\chi_1 \\sum_{j=1}^{\\infty} \\frac{z^j}{j!} + \\chi_2 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^2}{2} + \\chi_3 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^3}{3!} + \\cdots \\\\\n\\end{align}\n$$\nThen, by equating powers of $z$, we find\n$$\n\\begin{align}\n\\kappa_1 & = \\chi_1 \\\\\n\\kappa_2 & = \\chi_1 + \\chi_2 \\\\\n\\kappa_3 & = \\chi_1 + 3 \\chi_2 + \\chi_3 \\\\\n\\kappa_4 & = \\chi_1 + 7 \\chi_2 + 6 \\chi_3 + \\chi_4 \\\\\n         & \\cdots\n\\end{align}\n$$\nThis cumulant series has the intuitive property that if $X \\to \\lambda$ constant, $\\chi_\\alpha \\to \\lambda \\delta_{\\alpha, 1}$ and $\\kappa_\\alpha \\to \\chi_1$.\nThat is, $Y \\to \\text{Poisson}(\\lambda)$.\nFor random $X$, in constrast, all of the cumulants of $Y$ are increased from their Poisson value of $\\chi_1$ by the cumulants of $X$.\nIn particular, the variance of $Y$, $\\kappa_2$ is equal to the Poisson variance $\\chi_1$ plus the variance of $X$, $\\chi_2$.\n\n### Cumulants of the latent variable $X$\n\nIt remains to find the cumulants of $X$, $\\chi_\\alpha$.\nFor this, we need to specify a distribution for the latent variables at each sample, $X_i$.\nFor simplicity, we will choose the Gamma distribution, which allows us to vary the mean and coefficient of variation independently.\nWe will parameterize the distribution by its mean $\\mu$, and inverse dispersion $\\phi$.\nIn standard shape-scale parameterization, we have:\n$$\nX_i \\sim \\text{Gamma}(\\phi, \\mu_i / \\phi)\n$$\nwhere we assume that the inverse dispersion is constant in time.\nNote that the coefficient of variation of $X_i$ is $\\phi^{-1/2}$, independent of $\\mu_i$.\nThus our latent gamma model accounts for noise type 2 above.\n\nThe gamma distribution has CGF:\n$$\n\\begin{align}\nK_{X_i}(z) & = - \\phi \\log(1 - \\frac{\\mu_i}{\\phi} z) \\\\\n           & = \\phi \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha}\n\\end{align}\n$$\n\nBy the summation property of CGFs, we have the CGF of $X = \\sum_j X_j$ at time $t_i$:\n$$\n\\begin{align}\nK_{X}(z) & = \\sum_{j=0}^i K_{X_j}(z) \\\\\n         & = \\phi \\sum_{j=0}^i \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha} \\\\\n         & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left( \\sum_{j=0}^{i} \\mu_j^\\alpha \\right) z^\\alpha.\n\\end{align}\n$$\nBecause the prevalence is growing exponentially in our model, $\\mu_j$ is growing exponentially (see above).\nLetting $A \\equiv \\frac{nb}{N} e^{rt_0}$, we have $\\mu_j = A e^{rj\\delta t}$ and thus\n\n$$\n\\begin{align}\nK_X(z) & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left(\\sum_{j=0}^{i} A^\\alpha e^{\\alpha r j \\delta t} \\right) z^\\alpha \\\\\n       & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} A^\\alpha \\left(\\frac{e^{\\alpha r (i+1) \\delta t} - 1}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha \\left(\\frac{\\alpha r \\delta t e^{\\alpha r \\delta t}}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha z^\\alpha\n\\end{align}\n$$\nWhere the first approximation requires the prevalence to be large compared to one, and the second approximation requires $\\alpha r \\delta t \\ll 1$.\n(TODO: generalize the second approximation.)\n\nIt is illuminating to parameterize the distribution of $X$ by its mean, $\\mu$, and a shape parameter $\\nu$:\n$$\n\\begin{align}\n\\mu & \\equiv K_X'(0) \\\\\n    & = \\frac{A e^{r i \\delta t}}{r \\delta t}. \\\\\n\\nu & \\equiv \\frac{\\phi}{r \\delta t}.\n\\end{align}\n$$\nSubstituting these into the equation above gives\n$$\nK_X(z) = \\nu \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha^2} {\\left(\\frac{\\mu}{\\nu}\\right)}^\\alpha z^\\alpha.\n$$\n(Note that this is similar to the CGF of the gamma distribution but with the logarithm replaced by a dilogarithm.)\n\nFinally, examination of the CGF yields the cumulants of $X$:\n$$\n\\chi_\\alpha = \\frac{(\\alpha - 1)!}{\\alpha} \\nu {\\left(\\frac{\\mu}{\\nu}\\right)}^{\\alpha}.\n$$\n\nWe can say a few things about this result:\n\n- By construction, the mean of $X$, $\\mu$, is our previous deterministic prediction for the counts.\n  (In the small $r \\delta t$ limit).\n- The shape parameter $\\nu$ controls the dispersion of $X$. $\\text{Var}[X] = \\chi_2 = \\frac{\\mu^2}{2\\nu}$. That is: larger $\\nu$ means a smaller coefficient of variation.\n- $\\nu$ is controled by the latent inverse dispersion $\\phi$ and the scaled sampling interval $r \\delta t$. Smaller sampling inverval means we take more independent samples per unit time, which reduces the variance of the sum.\n- $\\frac{\\mu}{\\nu}$ is a pure scale parameter of the distribution of $X$.\n\n### Cumulants of the cumulative counts $Y$\n\nSubstituting our equation for the cumulants of $X$ $\\chi_\\alpha$ into the equation for the cumulants of $Y$ above gives\n$$\n\\begin{align}\n\\kappa_1 & = \\mu \\\\\n\\kappa_2 & = \\mu + \\frac{1}{2} \\frac{\\mu^2}{\\nu} \\\\\n\\kappa_3 & = \\mu + \\frac{3}{2} \\frac{\\mu^2}{\\nu} + \\frac{2}{3} \\frac{\\mu^3}{\\nu^2} \\\\\n\\kappa_4 & = \\mu + \\frac{7}{2} \\frac{\\mu^2}{\\nu} + 4 \\frac{\\mu^3}{\\nu^2} + \\frac{3}{2} \\frac{\\mu^4}{\\nu^3}. \\\\\n\\end{align}\n$$\n\nWe have two regimes, controled by the parameter $\\mu / \\nu$:\n\n- If $\\frac{\\mu}{\\nu} \\ll 1$, the Poisson noise dominates and $\\kappa_\\alpha \\approx \\mu$.\n- If $\\frac{\\mu}{\\nu} \\gg 1$, the latent noise dominates and $\\kappa_\\alpha \\approx \\chi_\\alpha$.\n\nFor higher cumulants, the separation between the regimes becomes less clean (i.e. it takes a smaller or larger $\\mu/\\nu$ for one term to dominate.)\n\nIn terms of model parameters:\n\n- More frequent samples (smaller $\\delta t$, thus larger $\\nu$) pushes us toward the Poisson-dominant regime.\n- More variable latent abundances (smaller $\\phi$, thus smaller $\\nu$) pushes us toward the latent-dominant regime.\n- A higher threshold of detection (larger $\\mu$ at the time of detection) pushes us toward the latent-dominant regime.\n\n## The Cornish-Fisher expansion of the quantiles of $Y$\n\nUltimately, our goal is to estimate the probability that $Y > \\hat{K}$, the detection threshold.\nThus, we need to estimate the CDF of $Y$ from its cumulants $\\kappa_\\alpha$.\nFor that we will use the Cornish-Fisher expansion.\nThe idea behind the expansion is to start by approximating the CDF as that of a Gaussian random variable with the correct mean and variance, and then iteratively adjust it for higher-order cumulants (skew, kurtosis, etc).\nIt is defined as follows:\n\nThe quantile function $y(p)$ (i.e. the value for which $\\text{CDF}(y) = p$) is approximated by $y(p) \\approx \\kappa_1 + {\\kappa_2}^{1/2} w_p$, where\n$$\n\\begin{align}\nw_p & = x \\\\\n    & + \\gamma_1 h_1(x) \\\\\n    & + \\gamma_2 h_2(x) + {\\gamma_1}^2 h_{11}(x) \\\\\n    & + \\cdots \\\\\nx   & = \\Phi^{-1}(p) \\\\\n\\gamma_{\\alpha-2} & = \\frac{\\kappa_\\alpha}{{\\kappa_2}^{\\alpha/2}} \\\\\nh_1(x) & = \\frac{\\text{He}_2(x)}{6} \\\\\nh_2(x) & = \\frac{\\text{He}_3(x)}{24} \\\\\nh_{11}(x) & = - \\frac{2\\text{He}_3(x) + \\text{He}_1(x)}{36} \\\\\n\\end{align}\n$$\nWhere $\\Phi$ is the CGF of the standard normal distribution and $\\text{He}$ are the probabilists' Hermite polynomials.\nNote that each line of the sum must be included as a whole for the approximation to be valid at that level.\n\n### Validity of the expansion\n\nFor fixed $x$ (and therefore fixed quantile), the relative sizes of the terms of the expansion are controlled by the coefficients $\\gamma$.\nIn the Poisson-dominated regime:\n$$\n\\begin{align}\n\\gamma_1 & = \\mu^{-1/2} \\\\\n\\gamma_2 & = \\gamma_1^2 = \\mu^{-1} \\\\\n\\end{align}\n$$\nso truncating the expansion at a few terms should work well when $\\mu > 1$.\nSince we're interested in having a significant probability of detection (e.g. >90%), and our threshold of detection is at least one read, this is not a very limiting requirement.\n\nIn the latent-noise-dominated regime:\n$$\n\\begin{align}\n\\gamma_1 & = \\frac{2^{5/2}}{3} \\nu^{-1/2} \\\\\n\\gamma_2 & = 6 \\nu^{-1}\n\\end{align}\n$$\nso truncating the series at a few terms should work well when $\\nu > 1$.\nThis is also not very limiting, because $\\nu$ can be large either by making $\\phi$ large (which is likely unless our data source is extremely noisy) or by sampling frequently so that $r \\delta t$ is small.\n\nPutting these together, we can see that for our purposes, the expansion will be good when $\\nu > 1$.\nWe're concerned with the value of $\\mu$ when a particular quantile (e.g. 5%) passes the detection threshold $\\hat{K}$\nThe detection threshold by definition must be at least one read.\nTherefore, at detection, $\\mu > 1$.\nIf $\\mu < \\nu$, the Poisson noise dominates we can use $\\mu > 1$ to establish the validity of the CF expansion.\nIf $\\mu > \\nu$, the latent noise dominates and we can use $\\nu > 1$ for validity.\n\n## Numerical calculations\n\n::: {#524ab633 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial.polynomial import Polynomial\nfrom numpy.polynomial.hermite_e import HermiteE\nfrom scipy.special import factorial\nfrom scipy.stats import poisson, norm, gamma\n```\n:::\n\n\n::: {#00232732 .cell execution_count=2}\n``` {.python .cell-code}\ndef cgf_x_series(mu: float, nu: float, order: int = 4) -> Polynomial:\n    coeffs = np.zeros(order + 1)\n    # No zeroth coefficient\n    k = np.arange(1, order + 1)\n    coeffs[1:] = nu * (mu / nu)**k / k**2\n    return Polynomial(coeffs)\n\ndef expm1_series(order: int = 4) -> Polynomial:\n    k = np.arange(order + 1)\n    return Polynomial(1.0 / factorial(k)) - 1\n\ndef cgf_y_series(mu: float, nu: float, order: int = 4):\n    return cgf_x_series(mu, nu, order)(expm1_series(order)).cutdeg(order)\n\ndef cumulant_from_cgf(cgf: Polynomial, order: int) -> float:\n    return cgf.deriv(order)(0)\n```\n:::\n\n\nSpot-check cumulants:\n\n::: {#24df2646 .cell execution_count=3}\n``` {.python .cell-code}\nmu = 2.0\nnu = 10.0\ncgf = cgf_x_series(mu, nu)(expm1_series()).cutdeg(4)\ncoeffs = [0, 1, (1, 1/2), (1, 3/2, 2/3), (1, 7/2, 4, 3/2)]\npredicted_cumulants = [mu * Polynomial(c)(mu / nu) for c in coeffs]\nfor k in range(5):\n    print(cumulant_from_cgf(cgf, k), predicted_cumulants[k])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0 0.0\n2.0 2.0\n2.2 2.2\n2.6533333333333333 2.6533333333333333\n3.7439999999999998 3.744\n```\n:::\n:::\n\n\nCheck variance:\n\n::: {#47e46155 .cell execution_count=4}\n``` {.python .cell-code}\nnu = 10.0\nmu = np.arange(1, 100)\nk2 = [cumulant_from_cgf(cgf_y_series(m, nu), 2) for m in mu]\nplt.plot(mu, k2, label=\"exact\")\nplt.plot(mu, (1/2) * mu**2 / nu, \"--\", label=\"latent approximation\")\nplt.xlabel(r\"$\\mu$\")\nplt.ylabel(r\"Variance of $Y$\")\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nNote that the variance is growing quadratically, but the Poisson contribution to the variance is not totally negligible even for large $\\nu$.\n\n### Checking the Cornish-Fisher expansion against common distributions\n\nIn this section, we compare the Cornish-Fisher expansion of the quantile function to the exact quantile function for several known distributions to get a sense of its accuracy at different orders.\n\n::: {#c1aae53e .cell execution_count=5}\n``` {.python .cell-code}\ndef cornish_fisher(*cumulants):\n    # cumulants = (k_1, k_2, ...)\n    order = len(cumulants)\n    if order < 2:\n        raise ValueError(\"Order of approximation must be >= 2\")\n    if order > 4:\n        raise ValueError(\"Order of approximation must be <= 4\")\n    sigma = np.sqrt(cumulants[1])\n    poly = HermiteE((0, 1))\n    if order >= 3:\n        gamma_1 = cumulants[2] / sigma**3\n        h_1 = HermiteE((0, 0, 1)) / 6\n        poly += gamma_1 * h_1\n    if order >= 4:\n        gamma_2 = cumulants[3] / sigma**4\n        h_2 = HermiteE((0, 0, 0, 1)) / 24\n        h_11 = - HermiteE((0, 1, 0, 2)) / 36\n        poly += gamma_2 * h_2 + gamma_1**2 * h_11\n    return cumulants[0] + sigma * poly\n```\n:::\n\n\nCheck against Poisson distribution:\n\n::: {#bd7631cb .cell execution_count=6}\n``` {.python .cell-code}\norder = 4\np = np.arange(0.01, 1.0, 0.01)\nx = norm.ppf(p)\n\nfor lamb in [1, 2, 4, 8]:\n    poisson_cumulants = [lamb] * order\n    for o in range(2, order + 1):\n        cf_poisson = cornish_fisher(*poisson_cumulants[:o])\n        plt.plot(p, cf_poisson(x), label=o)\n    plt.plot(p, poisson(lamb).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\")\n    plt.legend(title=\"order\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(f\"$\\lambda$ = {lamb}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:14: SyntaxWarning: invalid escape sequence '\\l'\n<>:14: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8544/3927045381.py:14: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f\"$\\lambda$ = {lamb}\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-5.png){}\n:::\n:::\n\n\nIn the Poisson case, when $\\lambda > 1$, the distribution converges quickly to a Gaussian, and the higher-order corrections don't matter.\n\nCheck against Gamma distribution:\n\n::: {#fb4f1140 .cell execution_count=7}\n``` {.python .cell-code}\nscale = 1\nk = np.arange(1, order + 1)\n\nfor shape in [1/2, 1, 2, 4, 8]:\n    gamma_cumulants = factorial(k - 1) * shape * scale ** k\n    for o in range(2, order + 1):\n        cf_gamma = cornish_fisher(*gamma_cumulants[:o])\n        plt.plot(p, cf_gamma(x), label=o)\n    plt.plot(p, gamma(shape, scale=scale).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\")\n    plt.legend(title=\"order\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(f\"shape = {shape}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-5.png){}\n:::\n:::\n\n\nThe shape parameter controls deviations from Gaussian and the error of the higher order approximations.\nFor shape > 1, three terms gets you close and 4 gets you very close.\nFor shape = 1/2, the Gaussian approximation is quite bad, but the order-4 Cornish fisher approximation is decent except for small quantiles. (It does not capture the power-law left tail of the distribution.)\n\n### Cornish-Fisher expansions for the cumulative count distribution\n\nFor the cumulative count distribution, there are two parameters, $\\mu$ (the mean) and $\\nu$ (which determines the shape).\n\nFor small $\\nu$, the noise is quickly dominated by the latent variable so the distribution goes to a constant shape, scaled by $\\mu$.\n\nEven with $\\nu = 2$, the CF expansion converges quickly. 3 terms is quite good, 4 is indistinguisable from higher:\n\n::: {#46961017 .cell execution_count=8}\n``` {.python .cell-code}\norder = 6\nnu = 2.0\nfor mu in [1, 2, 4, 8, 32]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend(title=\"order\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-5.png){}\n:::\n:::\n\n\nWith larger $\\nu$, the shape of the distribution is closer to Gaussian:\n\n::: {#7e423195 .cell execution_count=9}\n``` {.python .cell-code}\norder = 6\nnu = 10.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-4.png){}\n:::\n:::\n\n\nWith $\\nu = 100$, there is a wide Poisson-dominated regime the convergence to Gaussian is quite fast.\n\n::: {#ff9c7bc9 .cell execution_count=10}\n``` {.python .cell-code}\norder = 4\nnu = 100.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-4.png){}\n:::\n:::\n\n\n### Percentiles\n\nThe following show the mean (dashed grey lines) and the 10th percentile (black) of cumulative counts as a function of the mean $\\mu$.\nColored lines are the latent and poisson regime approximations.\nEach plot has a different shape parameter $\\nu$.\n\n::: {#8e6175a7 .cell execution_count=11}\n``` {.python .cell-code}\norder = 4\nks = range(1, order+1)\n# 10th percentile\np = 0.1\nx = norm.ppf(p)\n\nmus = np.arange(1, 100)\nfor nu in [4, 10, 100]:\n    cgfs = [cgf_y_series(mu, nu) for mu in mus]\n    cumulants = [[cumulant_from_cgf(cgf, k) for k in ks] for cgf in cgfs]\n    cumulants_latent = [[factorial(k-1) / k * nu * (mu / nu)**k for k in ks] for mu in mus]\n    cumulants_poisson = [[mu for _ in ks] for mu in mus]\n    cf = [cornish_fisher(*cs)(x) for cs in cumulants]\n    cf_l = [cornish_fisher(*cs)(x) for cs in cumulants_latent]\n    cf_p = [cornish_fisher(*cs)(x) for cs in cumulants_poisson]\n    plt.plot(mus, cf, 'k', label=\"full\")\n    plt.plot(mus, cf_l, color=\"C0\", label=\"latent\")\n    plt.plot(mus, cf_p, color=\"C1\", label=\"poisson\")\n    plt.plot(mus, mus, '--', color=\"grey\", label=\"mean\")\n    plt.xlabel(r\"$\\mu$\")\n    plt.ylabel(\"Cumulative counts\")\n    plt.title(r\"$\\nu = $\" f\"{nu}\")\n    plt.legend()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-3.png){}\n:::\n:::\n\n\nObservations:\n\n- Smaller $\\nu$ means that we have a greater reduction of the 10th percentile from the mean.\n- Smaller $\\nu$ means that the 10th percentile increase linearly (as expected in the latent regime).\n- As $\\nu$ gets larger, there is a wider Poisson-dominated regime where the tenth percentile increases like $\\mu + \\mu^{1/2} const.$.\n- Unless $\\nu$ is very large, we can probably get away with the latent-regime approximation\n\n## Implications for cost\n\nIn the Poisson-dominated regime, a Gaussian approximation is pretty good, so\n$$\ny(p) \\approx \\mu + \\mu^{1/2} \\Phi^{-1}(p)\n$$\nNote that this is true even when the detection threshold $\\hat{K} = 1$ because the mean will have to be larger than one to have a high probability of detection.\nWe can let $p$ be one minus the target probability of detection, set $y(p) = \\hat{K}$, and\nsolve for $\\mu$.\nThis allows us to calculate the delay in detection due to having to wait for the mean to be larger than the threshold.\n\nThe Poisson regime will be more appropriate for small read counts and thus small thresholds.\nLet's consider the effect of stochasticity on detection when the threshold $\\hat{K}$ is low.\nIf we detect when we observe two reads, the previous equation shows that we will need $\\mu \\geq 4.8$ to detect 10% of the time.\nIn contrast, our deterministic model predicts that we detect when $\\mu = 2$.\nThus, the Poisson noise costs us over a full doubling time in detection sensitivity.\n\n::: {#18064dbf .cell execution_count=12}\n``` {.python .cell-code}\nfrom scipy.optimize import fsolve\n\ndef mu_at_detection(k_hat, p):\n    def f(mu):\n        return mu + norm.ppf(p) * mu **(1/2) - k_hat\n    return fsolve(f, k_hat)[0]\n\nprint(mu_at_detection(2.0, 0.1))\n\nk_hat = np.arange(1, 20)\nmu_10 = np.array([mu_at_detection(k, 0.1) for k in k_hat])\nmu_05 = np.array([mu_at_detection(k, 0.05) for k in k_hat])\nplt.plot(k_hat, mu_10, \".\", label=\"Pr{detect}=0.1\")\nplt.plot(k_hat, mu_05, \".\", label=\"Pr{detect}=0.05\")\nplt.plot(k_hat, k_hat, \"--k\", label=\"deterministic\")\nplt.title(\"Poisson approximation\")\nplt.legend()\nplt.ylabel(r\"$\\mu$ at detection\")\nplt.xlabel(r\"Detection threshold $\\hat{K}$\")\nplt.show()\n\nplt.plot(k_hat, mu_10 / k_hat)\nplt.plot(k_hat, mu_05 / k_hat)\nplt.ylabel(\"Inflation factor relative to deterministic\")\nplt.xlabel(r\"Detection threshold $\\hat{K}$\")\nplt.ylim([1, 4.5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.810935246946805\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-3.png){}\n:::\n:::\n\n\nIn the Latent-dominated regime, the terms of the Cornish-Fisher expansion just depend on $\\nu$, not on $\\mu$, so:\n$$\n\\begin{align}\ny(p) & \\approx \\mu + \\frac{\\mu}{{(2\\nu)}^{1/2}} w_p(\\nu) \\\\\n     & = \\mu \\left(1 +\\frac{1}{{(2\\nu)}^{1/2}} w_p(\\nu)\\right) \\\\\n\\end{align}\n$$\nCan calculate $w_p(\\nu)$ to as high order as we need, then solve for $\\mu$ as in the Poisson regime.\nBecause $w_p$ will be negative (since $p$ is small), this will inflate the required $\\mu$ for detection by a factor.\nThis suggests a path for data analysis: if we can estimate $\\nu$ from data (by estimating the overdispersion parameter $\\phi$ in the read count distribution and then scaling by $r \\delta t$, we can estimate the inflation factor.\n\n::: {#e1e6932e .cell execution_count=13}\n``` {.python .cell-code}\ndef cf_latent(nu):\n    return HermiteE((1, (2*nu)**(-1/2), (2 / 9) / nu))\n\nnu = np.arange(1.0, 20.0)\nfactor_10 = np.array([cf_latent(n)(norm.ppf(0.1)) for n in nu])\nfactor_05 = np.array([cf_latent(n)(norm.ppf(0.05)) for n in nu])\nplt.plot(nu, 1 / factor_10, label=\"Pr{detect}=0.1\")\nplt.plot(nu, 1 / factor_05, label=\"Pr{detect}=0.05\")\nplt.legend()\nplt.ylabel(\"Inflation factor relative to deterministic\")\nplt.xlabel(r\"Shape parameter $\\nu$\")\nplt.ylim([1, 4.5])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nThis suggests that the deterministic approximation will be within a doubling when $\\nu \\gtrapprox 5$.\nHowever, it is never very close.\n\nPutting these results together suggests that regardless of the regime,\nwe expect the sequencing depth required to achieve detection probability of 0.9 or 0.95 at the target cumulative incidence to be roughly 1.5 to 3.0 times higher under a stochastic model than under the deterministic approximation.\n\nThe above assumes that $\\nu > 1.5$ or so.\nFor smaller $\\nu$, our approximations break down.\nHowever, smaller $\\nu$ means more variability.\nUnder those circumstances, the details of the distribution of sequencing outcomes likely matter in some detail.\n\n# Appendix: Small pool noise\n\nSo far we have assumed that our reads come from a homogenized sample of a large population as in wastewater.\nHowever, there is another potential source of noise when our sequenced sample comes from a small number of people such pooled samples from tens of nasal swabs.\nDepending on the sample collection details, aggregated airplane waste may also fit into this category.\nIn this case, we must account for noise from the random number of infected individual contributing to the sample.\nWe expect this noise to decrease as the size of the pool grows.\n\nAs above, let the number of viral reads in sample $i$ be $Y_i \\sim Poisson(X_i)$ with $X_i$ random.\nIf we sample a pool of $n_p$ individuals, each with an independent probability $p_i$ of  being infected, and each infected person expected to contribute $a$ viral reads to our sample, we have\n\n\\begin{align}\nK_{X_i}(z) & = n_p \\log \\left(1 + p_i (e^{a z} - 1) \\right) \\\\\n           & \\approx n_p p_i (e^{a z} - 1) \\\\\nK_{Y_i}(z) & \\approx n_p p_i \\left[\\exp \\left(a (e^z - 1)\\right) - 1 \\right]\n\\end{align}\n\nwhere the approximation is equivalent to a Poisson approximation to the number of sick people in the pool.\nSumming over samples and noting that $\\mu_i = a n_p p_i$, gives:\n\n\\begin{align}\nK_Y(z) & = \\sum_i K_{Y_i}(z) \\\\\n       & = \\mu \\left[ \\frac{\\exp \\left(a \\left(e^z - 1\\right)\\right) - 1}{a} \\right]\n\\end{align}\n\nExamining this, we see that the shape of the distribution is controlled by $a$, the average number of reads each sick person contributes to the sample:\n\n1. When $a \\ll 1$, $K_Y(z) \\approx \\mu \\left(e^z - 1\\right)$, i.e. the Poisson read count count noise dominates, as expected.\n2. When $a \\gg 1$, $K_Y(z) \\approx \\frac{\\mu}{a} \\left(e^{az} - 1\\right)$, i.e. the noise in the number of sick people in the pool dominates. (This is a scaled poisson random variable).\n\nWe have already examined case 1.\nIn case 2, the probability of detection is going to be determined by the probability of getting $\\lceil \\hat{K} / a \\rceil$ sick individuals in our pool.\nWhen $a > \\hat{K}$, any sick individual in the pool will probably contribute enough reads for detection.\n\nTo see the effect of a small pool, consider the the special case that we detect upon seeing one read ($\\hat{K} = 1$).\n(I believe that the conclusions here generalize to other \\hat{K}, but it's harder to show analytically.)\nIn that case, detection happens as soon as $Y > 0$:\n\n\\begin{align}\nPr\\{Y \\geq \\hat{K}\\} & = 1 - Pr\\{Y = 0\\} \\\\\n           & = 1 - \\exp \\left[ \\lim_{z \\to - \\infty} K_Y(z) \\right] \\\\\n           & = 1 - \\exp \\left[ -\\frac{\\mu}{a} (1 - e^{-a}) \\right]\n\\end{align}\n\nSay we want to detect with probability $1 - p_{miss}$ by the time the cumulative incidence reaches our target.\nWe'll reach this detection probability when\n\n$$\n\\mu =  \\left( \\frac{a}{1 - e^{-a}} \\right) (- \\log p_{miss}).\n$$\n\n::: {#0c0e9c7e .cell execution_count=14}\n``` {.python .cell-code}\na = np.arange(0.1, 10, 0.1)\nplt.plot(a, a / (1 - np.exp(-a)))\nplt.xlabel(r\"Viral reads per sick individual in pool, $a$\")\nplt.ylabel(r\"Inflation of $\\mu$ at detection\")\nplt.ylim([0, 10])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nThis shows that, all else equal, we prefer a larger pool to improve our consistency of detection.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}