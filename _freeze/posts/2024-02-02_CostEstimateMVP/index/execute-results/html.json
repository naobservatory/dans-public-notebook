{
  "hash": "3e655d33ba7f9fbbf8388546331590c1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NAO Cost Estimate MVP\"\nauthor: \"Dan Rice\"\ndate: 2024-02-02\nformat:\n  html:\n    code-fold: false\n    toc: true\njupyter: python3\nfilters:\n    - black-formatter\n---\n\n\n## Background\n\nSee [Google Doc](https://docs.google.com/document/d/1YAy4Dnvk7H5J7jOt7iCeBYqE6z3TdgHSr3jetimlJHo/edit)\nfor background.\nSee also [Simple detection cost model](https://docs.google.com/document/d/1aHboNUDrXWAmTmZGg9Ne8dyTUdEpsluafB48DJp1UN4/edit#heading=h.vijzo0j6vjkg) for P2RA version.\n\n## Epidemic model\n\n### Prevalence and incidence\n\nConsider a population of $N$ individuals.\nWe focus on the phase of the epidemic when it is growing exponentially.\nThis will be approximately true at an intermediate time when the virus is common enough\nthat we can neglect noise in reproduction, but rare enough that it is not running out of\nsusceptible people to infect.\nLet the rate of new infections per infected persion per unit time be $N \\alpha$ and the rate of recovery of an infected person to be $\\beta$.\nThen, we have the following ODE for the prevalence (the number of infected people, not the proportion of the population as in the P2RA report) $P(t)$:\n\n$$\n\\frac{dP}{dt} \\approx \\alpha N P - \\beta P.\n$$\n\nThe first term on the right-hand side is the incidence per unit time: $I \\approx \\alpha N P$.\n(Note that we are assuming that the number of susceptible people is approximately $N$, which will\ncease to be true as the virus becomes common.)\n\nSolving this equation and choosing $t = 0$ to be the time when one person is infected gives:\n$$\n\\begin{align}\nP(t) & = e^{(\\alpha N - \\beta) t} \\\\\n     & = e^{rt} \\\\\nI(t) & = \\alpha N e^{rt} \\\\\n     & = (r + \\beta) e^{rt},\n\\end{align}\n$$\nwhere we have defined the exponential growth rate $r \\equiv \\alpha N - \\beta$.\n\n### Cumulative incidence\n\nAs a yardstick for measuring the progress of the epidemic, we are interested in the *cumulative* incidence, $C(t)$, the number of people who have ever been infected.\nIntegrating $I(t)$ with the initial condition $C(0) = 1$ to account for the single individual infected at $t = 0$, we have:\n$$\n\\begin{align}\nC(t) & = 1 + \\int_0^t I(t') dt' \\\\\n     & = 1 + \\frac{r + \\beta}{r} (e^{rt} - 1).\n\\end{align}\n$$\nNote that when sick individuals never recover ($\\beta = 0$), $C(t) = P(t)$, as expected.\n\nA useful feature of the exponential growth regime is that all of our quantities of interest grow exponentially at the same rate and are thus in proportion to one another:\n$$\n\\begin{align}\nI(t) & = (r + \\beta) P(t) \\\\\nC(t) & \\sim \\frac{r + \\beta}{r} P(t), \\text{as } t \\to \\infty.\n\\end{align}\n$$\n\nRearranging, we see that the fraction of cumulative incidence that are currently infected is controled by the ratio of growth rate to recovery rate:\n$$\n\\frac{P(t)}{C(t)} \\sim \\frac{r}{r + \\beta}.\n$$\nThis ranges from zero when the epidemic grows slowly and individuals recover quickly, to one wher the epidemic grows much faster than recovery.\nFor a virus like SARS-CoV-2, where both doubling and recovery times were on the order of a week, we expect on the order of half of the cumulative infections to be currently sick during the exponetial phase.\n\n### Domain of validity\n\nAs described above, the approximation of deterministic exponential growth is valid when the number of infected individuals is large enough that the growth is roughly deterministic but small enough that most of the population is still susceptible.\nHere we'll approximate those bounds.\n\n#### Deterministic growth\n\nA simple stochastic model of the start of an epidemic is that infections and recoveries in a small increment of time $dt$ are independent of one another, that is:\n$$\n\\begin{align}\n\\text{\\# Births} & \\sim \\text{Poisson}(\\alpha N P dt) \\\\\n\\text{\\# Deaths} & \\sim \\text{Poisson}(\\beta P dt).\n\\end{align}\n$$\nThis implies that the change in prevalence, $dP = (\\text{\\# Births}) - (\\text{\\# Deaths})$, during $[t, t+dt)$ has mean and variance:\n$$\n\\begin{align}\n\\mathbb{E}(dP) & = (\\alpha N - \\beta) P dt \\\\\n               & = r P dt \\\\\n\\mathbb{V}(dP) & = (\\alpha N + \\beta) P dt \\\\\n               & = (r + 2 \\beta) P dt.\n\\end{align}\n$$\n\nA deterministic approximation is good when the coefficient of variation of $dP$ is much smaller than one, that is:\n$$\n\\begin{align}\n\\frac{\\sqrt{\\mathbb{V}(dP)}}{\\mathbb{E}(dP)} & \\ll 1 \\\\\n\\frac{\\sqrt{(r + 2 \\beta) P dt}}{r P dt} & \\ll 1 \\\\.\n\\end{align}\n$$\nSome algebra gives the condition that the prevalence is larger than a critical value:\n$$\nP \\gg \\frac{r + 2 \\beta}{r^2 dt}.\n$$\nIt remains to choose a suitable increment of time.\nWe expect the prevalence to change significantly ($\\mathbb{E}(\\frac{dP}{P}) = 1$) on the timescale $r^{-1}$ so $dt = r^{-1}$ is a reasonable choice as long as $r \\gtrsim \\beta$.\nThis gives the condition\n$$\nP \\gg \\frac{r + 2 \\beta}{r} = 1 + 2 \\beta / r.\n$$\nWhen this condition is met, we expect the epidemic to grow approximately deterministically at rate $r$.\n\nNote that this condition is violated at $t = 0$.\nWe must thus reinterpret $t = 0$ as the *effective* time that the epidemic would have infected a single person if our deterministic approximation for $P \\gg 1 + 2\\beta/r$ were extended backwards in time.\n[TODO: estimate the error in cumulative infections introduced by making this approximation.]\n\n#### Exponential growth\n\nAs the virus spreads, the number of susceptible people declines, reducing the rate of spread and resulting in sub-exponential growth.\nLet the number of susceptible people be $S(t)$, then the incidence per time is:\n$$\nI(t) = \\alpha S(t) P(t).\n$$\nAbove, we assumed that $S(t) = N$.\n$I(t)$ will thus be reduced by half by the time $S(t) = N / 2$.\nThis is a convenient upper bound on the domain of validity.\n\nAssuming that anyone previously infected is no longer susceptible, we have:\n$$\n\\begin{align}\nS(t) & \\gg N / 2 \\\\\nN - C(t) & \\gg N / 2 \\\\\nC(t) & \\ll N / 2.\n\\end{align}\n$$\n\nUsing $C = \\frac{r + \\beta}{r} P$ in the exponential regime, we can put this together with our lower bound of the deterministic approximation to get the region of validity:\n$$\n1 + 2\\beta / r \\ll P \\ll \\frac{N}{2 (1 + \\beta / r)}.\n$$\nThis shows that as long as the population is large ($N \\gg 1$), and the growth rate is not too slow compared to the recovery rate ($r \\gtrsim \\beta$), there will be a wide range of $P$ for which growth is approximately deterministic and exponential.\n\n## Sampling, sequencing, and detection\n\nConsider a set of samples taken at times $\\{t_0, t_1, \\ldots\\}$,\nwhere $t_0 \\ge 0$ is the the first sampling time after the virus begins to spread.\n(Presumably sampling has been ongoing but we'll ignore the earlier samples.)\nWe sequence sample $i$ to a total depth of $n_i$ reads\nand find that $k_i$ match the sequence of the pandemic virus.\n\nWe consider the virus to be detected when the cumulative number of viral reads reaches a threshold $\\hat{K}$.\nWe'll define $\\hat{t}$ to be the smallest sample time $t_i$ such that:\n$$\nK_i \\equiv \\sum_{j=0}^i k_j \\geq \\hat{K}.\n$$\nWe also want to consider the effect of the delay, $t_d$, between when the critical sample is collected and when we have processed, sequenced, and analyzed the data.\n\nWe will assess the success of a method by the (population-scaled) cumulative incidence at the time of detection, accounting for the delay:\n$$\nc(\\hat{t} + t_d) \\equiv C(\\hat{t} + t_d) / N\n$$\n\nWe're interested in taking samples from a large population ($N \\gg 1$).\nIn such a population, our ability to detect will depend not on the the raw number of people infected $P$, but on the proportion $p \\equiv P / N$.\nAt detection, $p$ will be at some typical magnitude that depends on our sensitivity but does not depend on $N$.\nWe can simplify our equations by neglecting terms that are $\\mathcal{O}(N^{-1})$ while keeping terms of order $p = e^{r t} / N$.\nThis is valid because we've already assumed that $e^{r t} \\gg 1$ so the latter are always larger than the former.\n\nUsing the equations above for cumulative incidence, we have the following simplification:\n$$\n\\begin{align}\nc(t) & = \\frac{1 + \\frac{r + \\beta}{r}\\left(e^{rt} - 1\\right)}{N} \\\\\n     & = \\frac{r + \\beta}{r} \\frac{e^{rt}}{N} + \\mathcal{O}(N^{-1}).\n\\end{align}\n$$\n\nNote that we can immediately see the cost of delay:\n$$\nc(\\hat{t} + t_d) = \\frac{r + \\beta}{r} \\frac{e^{r\\hat{t}}}{N} e^{r t_d}.\n$$\nWith an exponentially growing pandemic, post-sampling delay multiplies our cumulative incidence at detection by $e^{rt_d}$.\n\nIn the rest of this section, we answer the question:\n\n> If we want to detect the virus by the time it reaches a particular cumulative incidence, $\\hat{c}$,\nhow much sequencing do we need to do per unit time?\n\n### Sampling to relative abundance\n\nHere we make two more important deterministic assumptions:\n\n1. The viral read counts are deterministic, given the expected relative abundance, $a_i$, in each sample: $k_i = n_i a_i$.\n2. The expected relative abundance is itself a deterministic functional of the prevalence:\n$$\na_i = b \\int \\frac{P(t)}{N} \\rho_i(t) dt.\n$$\nHere, $\\rho_i$ is a density that says how much of sample $i$ was collected at different times,\nand $b$ is a fixed conversion factor between prevalence and relative abundance.\n(Note $b = RA_p(1)$ in the P2RA manuscript.)\n\nFrom here on, it will simplify things to specify two concrete sampling schemes.\nBoth schemes collect evenly-spaced samples with the same depth:\n\n* $n_i = n$\n* $t_i - t_{i-1} = \\delta t$\n* $0 \\leq t_0 < \\delta t$, since we don't know how long before the first sample the pandemic began.\n\nOur two schemes differ in when the material for the samples is collected.\nThey are:\n\n1. Continuous sampling: $\\rho_i(t) = \\frac{1}{\\delta t}$ for $t \\in [t_{i-1}, t_i)$.\n2. Grab sampling: $\\rho_i(t) = \\delta(t - t_i)$, i.e., the whole sample is collected at $t_i$. (Here, $\\delta(t)$ is the Dirac delta function.)\n\nIn the [next notebook](../2024-02-08_OptimalSamplingInterval/index.qmd#windowed-composite-sampling),\nwe consider a third, intermediate scheme: windowed composite sampling.\nIn this scheme, material is collected continuously for a fixed window, usually 24 hours.\n(In practice, it is not sampled continuously but at regular intervals over the window.)\nDerivation of this case is a straightforward extension of the cases considered here, using $\\rho_i(t) = w^{-1}$ for $t \\in [t_i - w, t_i)$ with window length $w$.\nFor short windows ($w \\to 0$), this case approaches grab sampling.\nFor $w \\to \\delta t$ it approaches composite sampling.\n\n### Continuous sampling\n\nFirst, we use the assumptions in the previous section to calculate the cumulative reads by sample $i$:\n$$\n\\begin{align}\nK_i & = \\sum_{j=0}^{i} n a_i \\\\\n    & = n \\sum_{j=0}^{i} b \\int_{t_{j-1}}^{t_j} \\frac{P(t)}{N} \\frac{dt}{\\delta t} \\\\\n    & = \\frac{n b}{\\delta t} \\int_{0}^{t_i} \\frac{P(t)}{N} dt.\n\\end{align}\n$$\n\nAt this point, we could make the substitution $I(t) = (r + \\beta) P(t)$ from our exponential model.\nThis would give us:\n$$\n\\begin{align}\nK_i & = \\frac{nb}{\\delta t} (r + \\beta) \\int_{0}^{t_i} \\frac{I(t)}{N} dt \\\\\n    & = \\frac{n b (r + \\beta)}{\\delta t} c(t_i),\n\\end{align}\n$$\nwhich is the result from the simple calculation in the P2RA manuscript,\nusing the conversion factor $r + \\beta$ to convert between $RA_p$ and $RA_i$,\nwhich is only valid in the exponential growth case.\n\nInstead, we'll proceed in a more general way, which will extend to the grab sampling model and account for the discrete nature of our sampling.\n\nUsing our exponential growth model for $P$, we have:\n$$\n\\begin{align}\nK_i & = \\frac{nb}{\\delta t}\n\\end{align}\n$$\n\nOne somewhat subtle complication comes from the fact that we don't know how the start of the epidemic lines up with our sampling schedule.\nFor example, if we sample weekly, the detection time will depend on which day of the week exponential growth effectively starts ($t=0$ in the model).\nWhen delay between the start of the epidemic and the first sample ($t_0$) is larger, we may eventually detect the virus one sample earlier than when the delay is smaller.\nIn the figure below, the three colored series show $K_i$ for three series of samples that differ only by the relative timing of their sample collection (e.g., all samples are collected weekly, but one series collects on Mondays, another on Tuesdays, etc.)\nNote that the red figure, whose first sample was taken latest relative to the start of the pandemic, crosses the threshold (solid black line) one sample earlier than the others.\n\n::: {#38746a32 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nt_max = 8\nr = 1.0\nk_hat = 100\ndt = 1.0\ntime = np.arange(0, t_max, 0.1)\nplt.plot(time, np.exp(r*time), 'k:')\nfor t_0 in [0.25, 0.5, 0.75]:\n    t = np.arange(t_0, t_max, dt)\n    k = np.exp(r * t)\n    plt.plot(t, k, 'o', label=t_0)\nplt.hlines(k_hat, 0, t_max, 'k')\nplt.yscale(\"log\")\nplt.ylabel(\"Cumulative reads $K$\")\nplt.xlabel(\"Time\")\nplt.title(\"Effect of sample timing\")\nplt.legend(title = \"$t_0$\");\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nWe account for this effect by splitting $\\hat{t}$ into two components:\n\n1. The earliest time possible to detect, $t^{\\star}$, found by allowing $K$ to vary continously in time (dotted line above)\n2. The residual time waiting for the next sample to be collected.\n   This waiting time depends on the arbitrary start time of the epidemic within our sampling cycle so we have reason to expect it to take any particular value.\n   Thus, we average it over a uniform distribution.\n\nFirst, we will find an implicit equation for $t^{\\star}$:\n$$\n\\begin{align}\n\\hat{K} & = \\frac{nb}{\\delta t} \\int_0^{t^{\\star}} \\frac{P(t)}{N} dt \\\\\n        & = \\frac{nb}{\\delta t} \\left(\\frac{e^{r t^{\\star}}}{rN} + \\mathcal{O}(N^{-1})\\right)\\\\\n\\end{align}\n$$\n\nNext we average our target cumulative incidence over the sample waiting time:\n$$\n\\begin{align}\n\\hat{c} & = \\int_{t^{\\star}}^{t^{\\star} + \\delta t} c(\\hat{t} + t_d) \\frac{d \\hat{t}}{\\delta t} \\\\\n        & = \\int_{t^{\\star}}^{t^{\\star} + \\delta t}\n                \\frac{r + \\beta}{r} \\frac{e^{r(\\hat{t} + t_d)}}{N} \\frac{d \\hat{t}}{\\delta t}\n                + \\mathcal{O}(N^{-1}) \\\\\n        & = (r + \\beta)\n            \\left(\\frac{e^{r t^{\\star}}}{r N} \\right)\n            \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n            e^{r t_d}\n            + \\mathcal{O}(N^{-1}) \\\\\n\\end{align}\n$$\n\nThis result has three components:\n\n1. The first two terms are the cumulative incidence at $t^{\\star}$, the earliest possible detection time.\n2. The third term in parentheses is the cost incurred from having widely spaced samples.\n   As $r \\delta t \\to 0$, this cost goes to zero.\n   When $r \\delta t$ is large, this cost grows exponentially.\n3. The final term is the multiplier from the delay between sample collection and analysis.\n\nFinally, we notice that the second term in parentheses appears in our implicit equation for $t^{\\star}$ above.\nSubstituting and rearranging gives the sequencing effort (reads per unit time, $n / \\delta t$) required to detect by cumulative incidence $\\hat{c}$:\n$$\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n    e^{r t_d}\n    + \\mathcal{O}(N^{-1})\n$$\n\nSome observations:\n\n1. Faster-growing and faster-recovering viruses require more sequencing because the current prevalence is a smaller fraction of the cumulative incidence.\n2. Higher detection thresholds, lower P2RA factors ($b$), and lower target cumulative incidence, require more sequencing.\n3. There is a cost associated with longer sampling intervals and delays between collection and analysis. The latter grows faster than the former.\n\n### Grab sampling\n\nNow we turn to grab sampling at the collection times $\\{t_0, t_1, \\ldots\\}$.\nThe analysis is the same as for continuous sampling, except that we will have a different\nimplicit equation for $t^{\\star}$.\n\nWith grab sampling, $\\rho_i(t) = \\delta(t - t_i)$, we have cumulative counts:\n$$\n\\begin{align}\nK_i & = n \\sum_{j=0}^{i} b \\frac{P(t_i)}{N} \\\\\n    & = \\frac{n b}{N} \\sum_{j=0}^{i} e^{r (t_0 + j \\delta t)} \\\\\n    & = \\frac{n b}{N} e^{r t_0} \\frac{e^{r(i+1)\\delta t} - 1}{e^{r\\delta t} - 1} \\\\\n    & = \\frac{n b}{\\delta t} \\left(\\frac{e^{r t_i}}{rN}\\right)\n        \\left(\\frac{r \\delta t \\, e^{r \\delta t}}{e^{r \\delta t} - 1}\\right)\n        + \\mathcal{O}(N^{-1}). \\\\\n\\end{align}\n$$\nMaking the continuous time substitution as before, gives\n$$\n\\hat{K} = \\frac{n b}{\\delta t} \\left(\\frac{e^{r t^{\\star}}}{rN}\\right)\n            \\left(\\frac{r \\delta t \\, e^{r \\delta t}}{e^{r \\delta t} - 1}\\right)\n            + \\mathcal{O}(N^{-1}).\n$$\nThe first two terms are identical to the continuous sampling case.\nThe third term is the effect of grab sampling: because the sample is collected entirely at the end of the interval, the prevalence is higher than the average over the interval and you get more reads.\n\nUsing this equation for $t^{\\star}$, we find that the required sequencing effort for grab sampling is:\n$$\n\\hat{c} = (r + \\beta)\n            \\left(\\frac{e^{r t^{\\star}}}{r N} \\right)\n            \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n            e^{r t_d}\n$$\n$$\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac\n        {e^{-r\\delta t} {\\left(e^{r \\delta t} - 1\\right)}^2}\n        {{\\left(r \\delta t\\right)}^2}\n        \\right)\n    e^{r t_d}\n    + \\mathcal{O}(N^{-1})\n$$\nThis is similar to the continous sampling case but with weaker dependence on the sampling interval:\n\n::: {#ec356c30 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nt = np.arange(0.01, 2, 0.01)\nplt.plot(t, (np.exp(t) - 1) / t, label=r\"Continuous sampling\")\nplt.plot(t, np.exp(-t) * ((np.exp(t) - 1) / t)**2, label=r\"Grab sampling\")\nplt.legend()\nplt.title(\"Effect of wider sampling intervals\")\nplt.xlabel(\"Sampling interval, $r \\delta t$\")\nplt.ylabel(\"Multiplicative effect on required sequencing depth\");\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:6: SyntaxWarning: invalid escape sequence '\\d'\n<>:6: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8448/798707106.py:6: SyntaxWarning: invalid escape sequence '\\d'\n  plt.xlabel(\"Sampling interval, $r \\delta t$\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){}\n:::\n:::\n\n\nWe can also see the difference between the cases by comparing the Taylor series of the sampling interval delay term:\n\\begin{align}\nf_{\\text{cont.}}(r \\delta t) & \\equiv \\frac{e^{r \\delta t} - 1}{r \\delta t} = 1 + \\frac{r \\delta t}{2} + \\frac{{(r \\delta t)}^2}{6} \\\\\nf_{\\text{grab}}(r \\delta t) & \\equiv \\frac{e^{- r \\delta t}{(e^{r \\delta t} - 1)}^2}{r \\delta t} = 1 + \\frac{{(r \\delta t)}^2}{12} \\\\\n\\end{align}\n\n## Numerical example\n\nA function that calculates the depth required per unit time to detect by a given cumulative incidence:\n\n::: {#0be6b438 .cell execution_count=3}\n``` {.python .cell-code}\ndef depth_required(\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_interval: float,\n    sampling_scheme: str,\n    delay: float,\n) -> float:\n    leading_term = (growth_rate + recovery_rate) * read_threshold / (p2ra_factor * cumulative_incidence_target)\n    x = growth_rate * sampling_interval\n    if sampling_scheme == \"continuous\":\n        sampling_term = (np.exp(x) - 1) / x\n    elif sampling_scheme == \"grab\":\n        sampling_term = np.exp(-x) * ((np.exp(x) - 1) / x)**2\n    else:\n        raise ValueError(\"sampling_scheme must be continuous or grab\")\n    delay_term = np.exp(growth_rate * delay)\n    return leading_term * sampling_term * delay_term\n```\n:::\n\n\nWe'll measure time in days.\nLet's assume:\n\n* A virus with a doubling time of 1 week and recovery timescale of two weeks\n* A read threshold of 100 viral reads\n* An $RA_i(1%)$ of 1e-7 (approximate median SARS-CoV2 in Rothman)\n* A cumulative incidence target of 1%\n* A delay of one week for sample processing and sequencing\n* Vary the sampling scheme and sampling interval (from daily to monthly)\n\n::: {#50670639 .cell execution_count=4}\n``` {.python .cell-code}\nr = np.log(2) / 7\nbeta = 1 / 14\nk_hat = 100\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\nc_hat = 0.01\ndelta_t = np.arange(1.0, 30, 1)\nt_d = 7.0\n\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nplt.plot(delta_t, 30 * n_cont, label=\"continuous\")\nplt.plot(delta_t, 30 * n_grab, label=\"grab\")\nplt.ylim([0, 5.1e10])\nplt.legend(title=\"Sampling scheme\")\nplt.ylabel(\"Reads required per month\")\nplt.xlabel(\"Sampling interval (days)\");\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n## Next steps\n\n- Vary the other parameters, make plots of interest?\n- Consider per-sample and per-read costs. Find the optimal sampling interval for each scheme.\n- Relax deterministic assumptions:\n  - Poisson read count noise\n  - \"Excess\" read count noise\n  - Randomness in the spread of the virus\n- Consider multiple sites\n  - Accounting for global spread\n  - Determining how to spread monitoring effort across locations\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}