{
  "hash": "769ced47572d68c42e7ad0844e26e931",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NAO Cost Estimate MVP -- Optimizing the sampling interval\"\nauthor: \"Dan Rice\"\ndate: 2024-02-13\nformat:\n  html:\n    code-fold: false\n    toc: true\njupyter: python3\nfilters:\n    - black-formatter\n---\n\n\n## Background\n\nSee [previous notebook](../2024-02-02_CostEstimateMVP/index.qmd).\nThe goal of this notebook is to use our simple, deterministic cost estimate to answer the question:\n\n> How often should we process and sequence samples?\n\nWe want to understand the tradeoff between:\n\n1. catching the virus earlier by sampling more frequently, and\n1. saving on processing costs by sampling less frequently.\n\nTo this end, we posit a two-component cost model:\n\n* Per-read sequencing costs, and\n* Per-sample processing costs\n\nand find the optimal sampling interval $\\delta t$ that minimizes total costs,\nwhile sequencing to sufficient depth per sample $n$ to detect a virus by cumulative incidence $\\hat{c}$.\n\n## A two-component cost model\n\nConsider the cost averaged over a long time interval $T$ in which we will take many samples.\nIf we collect and process samples every $\\delta t$ days, we will take $T / \\delta t$ samples in this interval.\nIf we sample $n$ reads per sample, our total sequencing depth is $n \\frac{\\delta t}{T}$ reads.\nAssume that our costs can be divided into a per-sample cost $d_s$ (including costs of collection, transportation, and processing for sequencing) and a per-read cost $d_r$ of sequencing.\n(Note: the $d$ is sort of awkward because we've already used $c$ for \"cumulative incidence\".\nYou can think of it as standing for \"dollars\".)\n\nWe will seek to minimize the total cost of detection:\n$$\nd_{\\text{tot}} = d_s \\frac{T}{\\delta t} + d_r \\frac{nT}{\\delta t}.\n$$\nEquivalently, we can divide by the arbitrary time-interval $T$ to get the total rate of spending:\n$$\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + d_r \\frac{n}{\\delta t}.\n$$\n\nIn our [previous post](../2024-02-02_CostEstimateMVP/index.qmd),\nwe found that the read depth per time required to detect a virus by the time it reaches cumulative incidence $\\hat{c}$ is:\n\n$$\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} f(r \\delta t)\n$$\n\nwhere the function $f$ depends on the sampling scheme.\nSubstituting this into the rate of spending, we have:\n$$\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} d_r f(r \\delta t).\n$$\n\nIn the next section, we will find the value of $\\delta t$ that minimizes the rate of spending.\n\n### Limitations of the two-component model\n\n* We assume that we process each sample as it comes in. In practice, we could stockpile a set of $m$ samples and process them simultaneously.\n  This would require splitting out the cost of sampling from the cost of sample prep.\n* We do not consider the fact that sequencing (and presumably to some extent sample prep) unit costs decrease with greater depth.\n  (I.e., it's cheaper per-read to do bigger runs.)\n* We neglect the \"batch\" effects of sequencing. Typically you buy sequencing in units of \"lanes\" rather than asking for an arbitrary number of reads. This will introduce threshold effects, where we want to batach our samples to use lanes efficiently.\n* We do not account for fixed costs that accumulate per unit time regardless of our sampling and sequencing protocols.\n  These do not affect the optimization here, but they do add to the total cost of the system.\n\n## Optimizing the sampling interval\n\nTo find the optimal $\\delta t$, we look for a zero of the derivative of spending rate:\n\n$$\n\\begin{align}\n\\frac{d}{d \\delta t} \\frac{d_{\\text{tot}}}{T} & = - \\frac{d_s}{{\\delta t}^2} + (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} d_r r f'(r\\delta t).\n\\end{align}\n$$\n\nSetting the right-hand side equal to zero and rearranging gives:\n\n$$\n{(r \\delta t)}^2 f'(r \\delta t) = \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}\n$$\n\nTo get any farther, we need to specify $f$ and therefore a sampling scheme.\n[Note: If we give some general properties of $f$, we can say some things here that are general to the sampling scheme]\n\n### Grab sampling\n\nWe first consider grab sampling, where the entire sample is collected at the sampling time.\nIn that case, we have:\n$$\n\\begin{align}\nf(x) & = \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & = 1 + \\frac{x^2}{12} + \\mathcal{O}(x^3).\n\\end{align}\n$$\nWe are particularly interested in the small-$x$ regime:\nThe depth required becomes exponentially large when $r \\delta t \\gg 1$,\nso it is likely that the optimal interval satisfies $r \\delta t \\lesssim 1$.\nWe can check this for self-consistency in any specific numerical examples.\n\nThis gives us the derivative:\n$$\nf'(x) \\approx \\frac{x}{6}.\n$$\n\nUsing this in our optimization equation yields:\n$$\n{(r \\delta t)}^3 \\approx 6 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n$$\n\n### Continuous sampling\n\nIn the case of continuous sampling, where the sample taken at time $t$ is a composite sample uniformly collected over the interval $[t - \\delta t, t)$, we have:\n$$\n\\begin{align}\nf(x) & = \\frac{e^x - 1}{x} \\\\\n     & = 1 + \\frac{x}{2} + \\mathcal{O}(x^2) \\\\\nf'(x) & \\approx \\frac{1}{2}\n\\end{align}\n$$\nfor small $x$.\nNote the difference in functional form from the grab sample case.\n\nSubstituting this into the optimization equation yields:\n$$\n{(r \\delta t)}^2 \\approx 2 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n$$\n\n### Windowed composite sampling\n\nAn intermediate (and more realistic) model of sampling is windowed composite sampling.\nIn this scheme, the sample at time $t$ is a composite sample taken over a window of width $w$ (e.g., 24hours) from $t - w$ to $t$.\nNotably, when the sampling interval ($\\delta t$) increases, the length of the window does not.\nIn this case we have:\n\n$$\n\\begin{align}\nf(x) & = \\frac{rw}{1 - e^{-rw}} \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & \\approx \\left(1 + \\frac{rw}{2}\\right) \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & \\approx \\left(1 + \\frac{rw}{2}\\right) + \\left(1 + \\frac{rw}{2}\\right) \\left(\\frac{x^2}{12}\\right) \\\\\nf'(x) & \\approx \\left(1 + \\frac{rw}{2}\\right) \\left(\\frac{x}{6}\\right) \\\\\n\\end{align}\n$$\nfor small $rw$ and $x$.\nNote that as $rw \\to 0$, we recover grab sampling.\n\nSince we're keeping only the leading term in $x = r \\delta t$, and $w \\leq \\delta t$,\nfor consistency we should also drop the $\\frac{rw}{2}$ (or keep more terms of the expansion).\nThus, we'll treat windowed composite sampling for small windows as equivalent to grab sampling.\nThe key reason for this is that changing the sampling interval does not change the window.\nNote that for $\\delta t \\approx w$, i.e. $x \\approx rw$, $f(x) \\approx 1 + \\frac{rw}{2}$, just as with continuous sampling, but $f'(x)$ still behaves like grab sampling.\n\n### General properties\n\nIn general, we have:\n$$\nr \\delta t \\approx {\\left( a\\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d} \\right)}^{1 / \\gamma},\n$$\nwhere $a$ and $\\gamma$ are positive constants that depend on the sampling scheme.\nWe can observe some general features:\n\n* Faster-growing viruses (higher $r$) **decreases** the optimal sampling interval.\n* Increasing the cost per sample $d_s$ **increases** the optimal sampling interval.\n* Increasing the cost per read $d_r$ **decreases** the optimal sampling interval.\n* Increasing the P2RA factor $b$ or the target cumulative incidence $c$ **increases** the optimal sampling interval.\n* Increasing the detection threshold $\\hat{K}$ **decreases** the optimal sampling interval.\n* Increasing the delay between sampling and detection $t_d$ **decreases** the optimal sampling interval.\n\nOne general trend is: the more optimistic we are about our method (higher $b$, smaller $\\hat{K}$, shorter $t_d$), the longer we can wait between samples.\n\nWe can also substitute our equation for $n / \\delta t$ into this equation, use $f(\\delta t) \\approx 1$ and rearrange to get:\n$$\nn d_r \\approx \\frac{a}{{(r \\delta t)}^{\\gamma - 1}} d_s.\n$$\nThe left-hand side of this equation is the cost spent on sequencing per sample.\nFor continuous sampling, $\\gamma = 2$ and for grab sampling and windowed composite, $\\gamma = 3$.\nSince $r \\delta t \\ll 1$, this tells us that we typically should spend more money on sequencing than sample processing.\n\n## A numerical example\n\n### Optimal $\\delta t$\n\n::: {#94e29ff8 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Optional\n\ndef optimal_interval(\n    per_sample_cost: float,\n    per_read_cost: float,\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_scheme: str,\n    delay: float,\n    composite_window: Optional[float] = None,\n) -> float:\n    constant_term = (\n        (per_sample_cost / per_read_cost)\n        * ((p2ra_factor * cumulative_incidence_target) / read_threshold)\n        * (growth_rate / (growth_rate + recovery_rate))\n        * np.exp(- growth_rate * delay)\n    )\n    if sampling_scheme == \"continuous\":\n        a = 2\n        b = 2\n    elif sampling_scheme == \"grab\":\n        a = 6\n        b = 3\n    elif sampling_scheme == \"composite\":\n        if not composite_window:\n            raise ValueError(\"For composite sampling, must provide a composite_window\")\n        a = 6 * (1 - np.exp(-growth_rate * composite_window)) / (growth_rate * composite_window)\n        b = 3\n    else:\n        raise ValueError(\"sampling_scheme must be continuous or grab\")\n    return (a * constant_term)**(1 / b) / growth_rate\n```\n:::\n\n\nI asked the NAO in [Twist](https://twist.com/a/197793/ch/565514/t/5896609/) for info on sequencing and sample-processing costs.\nBased on their answers, reasonable order-of-magnitude estimate are:\n\n    - sample costs: $500 / sample\n    - sequencing costs: $5K / billion reads\n\nNote that 1 billion reads cost roughly 10x the cost to prepare one sample.\nAs discussed above, our cost model is highly simplified and the specifics of when samples are collected, transported, processed, and batched for sequencing will make this calculation much more complicated in practice.\n\nLet's use these numbers plus our virus model from the [last post](../2024-02-02_CostEstimateMVP/index.qmd#numerical-example) to find the optimal sampling interval:\n\n::: {#e1b36a38 .cell execution_count=2}\n``` {.python .cell-code}\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Weekly doubling\nr = np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n# Delay from sampling to detecting of 1 week\nt_d = 7.0\n\ndelta_t_grab = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\ndelta_t_cont = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"continuous\", t_d)\ndelta_t_24hr = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1)\n\nprint(f\"Optimal sampling interval with grab sampling:\\t\\t{delta_t_grab:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_grab:.2f}\")\nprint(f\"Optimal sampling interval with continuous sampling:\\t{delta_t_cont:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_cont:.2f}\")\nprint(f\"Optimal sampling interval with 24-hour composite sampling:\\t{delta_t_24hr:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_24hr:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal sampling interval with grab sampling:\t\t5.98 days\n\tr delta_t = 0.59\nOptimal sampling interval with continuous sampling:\t2.66 days\n\tr delta_t = 0.26\nOptimal sampling interval with 24-hour composite sampling:\t5.89 days\n\tr delta_t = 0.58\n```\n:::\n:::\n\n\nWe should check that $r \\delta_t$ is small enough that our approximation for $f(x)$ is accurate:\n\n::: {#27b8212d .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nx = np.arange(0.01, 3, 0.01)\nplt.plot(x, np.exp(-x) * ((np.exp(x) - 1) / x)**2, label=\"exact\")\nplt.plot(x, 1 + x**2 / 12, label=\"approx\")\nplt.ylim([0,2])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Grab/24hr-composite sampling\")\nplt.show()\n\nplt.plot(x, (np.exp(x) - 1) / x, label=\"exact\")\nplt.plot(x, 1 + x / 2, label=\"approx\")\nplt.ylim([0,5])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Continuous sampling\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\nLooks fine in both cases.\n\n### Cost sensitivity to $\\delta t$\n\nIn a real system, we won't be able to optimize $\\delta t$ exactly.\nLet's see how the cost varies with the sampling interval (using the exact $f$):\n\n::: {#c53be080 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\ndef depth_required(\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_interval: float,\n    sampling_scheme: str,\n    delay: float,\n    composite_window: Optional[float] = None,\n) -> float:\n    leading_term = (\n        (growth_rate + recovery_rate)\n        * read_threshold\n        / (p2ra_factor * cumulative_incidence_target)\n    )\n    x = growth_rate * sampling_interval\n    if sampling_scheme == \"continuous\":\n        sampling_term = (np.exp(x) - 1) / x\n    elif sampling_scheme == \"grab\":\n        sampling_term = np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n    elif sampling_scheme == \"composite\":\n        if not composite_window:\n            raise ValueError(\"For composite sampling, must provide a composite_window\")\n        rw = growth_rate * composite_window\n        sampling_term = (rw / (1 - np.exp(-rw))) * np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n    else:\n        raise ValueError(\"sampling_scheme must be continuous, grab, or composite\")\n    delay_term = np.exp(growth_rate * delay)\n    return leading_term * sampling_term * delay_term\n\ndef cost_per_time(\n    per_sample_cost: float,\n    per_read_cost: float,\n    sampling_interval: float,\n    sample_depth_per_time: float,\n) -> float:\n    return (\n        sample_cost_per_time(per_sample_cost, sampling_interval)\n        + seq_cost_per_time(per_read_cost, sample_depth_per_time)\n    )\n\ndef sample_cost_per_time(per_sample_cost, sampling_interval):\n    return per_sample_cost / sampling_interval\n\ndef seq_cost_per_time(per_read_cost, sample_depth_per_time):\n    return per_read_cost * sample_depth_per_time\n```\n:::\n\n\n::: {#f5ea11ba .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\ndelta_t = np.arange(1.0, 21, 1)\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nn_24hr = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"composite\", t_d, composite_window=1.0)\ncost_cont = cost_per_time(d_s, d_r, delta_t, n_cont)\ncost_grab = cost_per_time(d_s, d_r, delta_t, n_grab)\ncost_24hr = cost_per_time(d_s, d_r, delta_t, n_24hr)\nplt.plot(delta_t, cost_cont, label=\"continuous\")\nplt.plot(delta_t, cost_grab, label=\"grab\")\nplt.plot(delta_t, cost_24hr, label=\"24hr composite\")\nplt.ylim([0, 5000])\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend();\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nFirst, note that the cost of 24hr composite sampling is quite close to grab sampling, and that\nwhen the sampling interval is 1 day, it is exactly the same as continuous sampling.\n\nIt looks like the cost curve is pretty flat for the grab/24hr sampling, suggesting that we could choose a range of sampling intervals without dramatically increasing the cost.\nFor continuous sampling, the cost increases more steeply with increasing sampling interval.\n\nFinally, let's break the costs down between sampling and sequencing:\n\n::: {#0c6b20a7 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(delta_t, cost_grab, label=\"Total\")\nplt.plot(delta_t, sample_cost_per_time(d_s, delta_t), label=\"Sampling\")\nplt.plot(delta_t, seq_cost_per_time(d_r, n_grab), label=\"Sequencing\")\nplt.legend()\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.title(\"Grab sampling\");\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nWe can observe a few things:\n\n* Sequencing costs are always quite a bit higher than sampling costs.\n* Increasing the sampling interval from one day to about five generates a significant savings in sampling cost, any longer than that gives strongly diminishing returns.\n  (This makes sense from the functional form $d_s / \\delta t$.)\n* The required sequencing depth increases slowly in this range.\n\n### Sensitivity of optimal $\\delta t$ to P2RA factor\n\nWe have a lot of uncertainty in the P2RA factor, even for a specific known virus with a fixed protocol.\nLet's see how the optimal sampling interval varies with it.\n(We'll only do this for grab sampling.)\n\n::: {#4b8c3db8 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nra_i_01 = np.logspace(-9, -6, 100)\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\n\nplt.semilogx(ra_i_01, delta_t_opt)\nplt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\nplt.ylabel(\"Optimal sampling interval, $\\delta t$\")\nplt.ylim([0, 13]);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:8: SyntaxWarning: invalid escape sequence '\\%'\n<>:9: SyntaxWarning: invalid escape sequence '\\d'\n<>:8: SyntaxWarning: invalid escape sequence '\\%'\n<>:9: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/3336883113.py:8: SyntaxWarning: invalid escape sequence '\\%'\n  plt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/3336883113.py:9: SyntaxWarning: invalid escape sequence '\\d'\n  plt.ylabel(\"Optimal sampling interval, $\\delta t$\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\nAs expected, the theory predicts that with higher P2RA factors, we can get away with wider sampling intervals.\nAlso, for this range of P2RA factors, it never recommends daily sampling.\n\nHowever, we can also see that the cost per day depends much more strongly on the P2RA factor than on optimizing the sampling interval:\n\n::: {#79d7ac64 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\ndelta_t = np.arange(1.0, 21, 1)\nfor ra_i_01 in [1e-8, 1e-7, 1e-6]:\n    b = ra_i_01 * 100 * (r + beta) * 7\n    n = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\n    cost = cost_per_time(d_s, d_r, delta_t, n)\n    plt.plot(delta_t, cost, label=f\"{ra_i_01}\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend(title=r\"$RA_i(1\\%)$\");\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\n## A second example: Faster growth and longer delay\n\nLet's consider a more pessimistic scenario: doubling both the growth rate and the delay to detection.\n\n::: {#0d19c838 .cell execution_count=9}\n``` {.python .cell-code}\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Twice-weekly doubling\nr = 2 * np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n# Delay from sampling to detecting of 2 weeks\nt_d = 14.0\n\ndelta_t_grab = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\ndelta_t_cont = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"continuous\", t_d)\ndelta_t_24hr = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1)\n\nprint(f\"Optimal sampling interval with grab sampling:\\t\\t{delta_t_grab:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_grab:.2f}\")\nprint(f\"Optimal sampling interval with continuous sampling:\\t{delta_t_cont:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_cont:.2f}\")\nprint(\n    f\"Optimal sampling interval with 24-hour composite sampling:\\t{delta_t_24hr:.2f} days\"\n)\nprint(f\"\\tr delta_t = {r*delta_t_24hr:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal sampling interval with grab sampling:\t\t1.88 days\n\tr delta_t = 0.37\nOptimal sampling interval with continuous sampling:\t0.66 days\n\tr delta_t = 0.13\nOptimal sampling interval with 24-hour composite sampling:\t1.82 days\n\tr delta_t = 0.36\n```\n:::\n:::\n\n\nWe should check that $r \\delta_t$ is small enough that our approximation for $f(x)$ is accurate:\n\n::: {#5fb5275e .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nx = np.arange(0.01, 3, 0.01)\nplt.plot(x, np.exp(-x) * ((np.exp(x) - 1) / x)**2, label=\"exact\")\nplt.plot(x, 1 + x**2 / 12, label=\"approx\")\nplt.ylim([0,2])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Grab sampling\")\nplt.show()\n\nplt.plot(x, (np.exp(x) - 1) / x, label=\"exact\")\nplt.plot(x, 1 + x / 2, label=\"approx\")\nplt.ylim([0,5])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Continuous sampling\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){}\n:::\n:::\n\n\nLooks fine in both cases.\n\n### Cost sensitivity to $\\delta t$\n\nIn a real system, we won't be able to optimize $\\delta t$ exactly.\nLet's see how the cost varies with the sampling interval:\n\n::: {#4a537bb3 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\ndelta_t = np.arange(1.0, 21, 1)\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nn_24hr = depth_required(\n    r, beta, k_hat, b, c_hat, delta_t, \"composite\", t_d, composite_window=1.0\n)\ncost_cont = cost_per_time(d_s, d_r, delta_t, n_cont)\ncost_grab = cost_per_time(d_s, d_r, delta_t, n_grab)\ncost_24hr = cost_per_time(d_s, d_r, delta_t, n_24hr)\nplt.plot(delta_t, cost_cont, label=\"continuous\")\nplt.plot(delta_t, cost_grab, label=\"grab\")\nplt.plot(delta_t, cost_24hr, label=\"24hr composite\")\nplt.ylim([0, 100000])\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend();\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\nIt looks like the cost curve is pretty flat for the grab sampling, suggesting that we could choose a range of sampling intervals without dramatically increasing the cost.\nFor continuous sampling, the cost increases more steeply with increasing sampling interval.\n\nFinally, let's break the costs down between sampling and sequencing:\n\n::: {#d2e4f12f .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(delta_t, cost_grab, label=\"Total\")\nplt.plot(delta_t, sample_cost_per_time(d_s, delta_t), label=\"Sampling\")\nplt.plot(delta_t, seq_cost_per_time(d_r, n_grab), label=\"Sequencing\")\nplt.legend()\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.title(\"Grab sampling\");\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\nIn this faster growth + more delay example, sequencing costs completely dwarf sampling costs.\n\n### Sensitivity of optimal $\\delta t$ to P2RA factor\n\n::: {#04fc9a66 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nra_i_01 = np.logspace(-9, -6, 100)\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\n\nplt.semilogx(ra_i_01, delta_t_opt)\nplt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\nplt.ylabel(\"Optimal sampling interval, $\\delta t$\")\nplt.ylim([0, 5]);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:8: SyntaxWarning: invalid escape sequence '\\%'\n<>:9: SyntaxWarning: invalid escape sequence '\\d'\n<>:8: SyntaxWarning: invalid escape sequence '\\%'\n<>:9: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/223304510.py:8: SyntaxWarning: invalid escape sequence '\\%'\n  plt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/223304510.py:9: SyntaxWarning: invalid escape sequence '\\d'\n  plt.ylabel(\"Optimal sampling interval, $\\delta t$\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){}\n:::\n:::\n\n\nIn this case, daily sampling is sometimes favored when the P2RA factor is small enough.\n\nHowever, we can also see that the cost per day depends much more strongly on the P2RA factor than on optimizing the sampling interval:\n\n::: {#005b6e21 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\ndelta_t = np.arange(1.0, 21, 1)\nfor ra_i_01 in [1e-8, 1e-7, 1e-6]:\n    b = ra_i_01 * 100 * (r + beta) * 7\n    n = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\n    cost = cost_per_time(d_s, d_r, delta_t, n)\n    plt.plot(delta_t, cost, label=f\"{ra_i_01}\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend(title=r\"$RA_i(1\\%)$\");\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n## Cost sensitivity to the latency, $t_d$\n\nAs a final application, let's calculate what the optimal cost would be as a function\nof delay/latency time $t_d$.\nWe'll use 24-hr composite sampling.\nAnd for some realism, we'll round the optimal sampling interval to the nearest day.\n\n::: {#127df79a .cell execution_count=15}\n``` {.python .cell-code}\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Bi-weekly doubling\nr = 2 * np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n```\n:::\n\n\n::: {#fa72f171 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\nt_d = np.arange(1.0, 22.0, 1.0)\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1.0)\ndelta_t_round = np.round(delta_t_opt)\nn = depth_required(r, beta, k_hat, b, c_hat, delta_t_round, \"composite\", t_d, 1.0)\ncost = cost_per_time(d_s, d_r, delta_t_round, n)\n\nplt.plot(t_d, delta_t_round, 'o')\nplt.ylim([0, 5])\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(r\"Optimal sampling interval $\\delta t$ (days)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\nShorter latency means that we can sample less often.\n\n::: {#c5db26c3 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(t_d, n, 'o')\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(\"Depth per day (reads)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\nLonger latency means that we have to sequence exponentially more reads per day.\nThis leads to exponentially higher costs:\n\n::: {#501b2535 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(t_d, cost, 'o')\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(\"Cost per day (dollars)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}