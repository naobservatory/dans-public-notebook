{
  "hash": "5f29d12ccdd6f4680ad0ecac934f5718",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NAO Cost Estimate -- Summary\"\nauthor: \"Dan Rice\"\ndate: 2024-03-25\nformat:\n  html:\n    code-fold: false\n    toc: true\njupyter: python3\nfilters:\n    - black-formatter\n---\n\n\n# Background\n\nThe goal of this project was to build a model that allows us to:\n\n- Estimate the cost of sampling and sequencing required to run an effective NAO.\n- Calculate the sequencing depth necessary to detect a virus by the time it reaches a target cumulative incidence.\n- Understand which parameters are most important to understand and/or optimize to determine the viability of an NAO.\n\nWe had previously done a very simple version of this for the P2RA project.\nHere, we wanted to formalize the approach and include more details.\n\nPrevious documents:\n\n1. [NAO Cost Estimate Outline](https://docs.google.com/document/d/1_DwBbE3l08ccbDiX0kxZwWqwiNm2CauHTQeDvlEaFRI)\n1. [Simple detection cost for P2RA](https://docs.google.com/document/d/1aHboNUDrXWAmTmZGg9Ne8dyTUdEpsluafB48DJp1UN4)\n2. [NAO Cost Estimate MVP outline and assumptions](https://docs.google.com/document/d/1YAy4Dnvk7H5J7jOt7iCeBYqE6z3TdgHSr3jetimlJHo)\n3. [NAO Cost Estimate MVP](../2024-02-02_CostEstimateMVP/index.qmd)\n4. [NAO Cost Estimate -- Optimizing the sampling interval](../2024-02-08_OptimalSamplingInterval/index.qmd)\n5. [NAO Cost Estimate -- Adding noise](../2024-02-22_StochasticMVP/index.qmd)\n\n# The model\n\nUsing the framework developed in [NAO Cost Estimate Outline](https://docs.google.com/document/d/1_DwBbE3l08ccbDiX0kxZwWqwiNm2CauHTQeDvlEaFRI),\nour model has the following components.\nUnless otherwise noted, see\n[NAO Cost Estimate MVP](../2024-02-02_CostEstimateMVP/index.qmd)\nfor details.\n\n## Epidemic\n\nThe prevalence of the virus grows exponentially and deterministically in a single population.\nThe fraction of people currently infectious and shedding is equal and given by:\n$$\np(t) = \\frac{1}{N} e^{r t},\n$$\nwhere $N$ is the population size and $r$ is the growth rate.\n\nThe cumulative incidence (as a fraction of the population) in this model is:\n$$\nc(t) \\approx \\frac{r + \\beta}{r} p(t),\n$$\nwhere $\\beta$ is the rate at which infected people recover.\nNote that both prevalence and cumulative incidence grow exponentially,\nwhich is convenient for many of our calculations.\n\n## Data collection\n\nWe collect samples from a single sampling site at regular intervals, spaced $\\delta t$ apart.\nThe material for the sample is collected uniformly over a window of length $w$.\n(When $w \\to 0$, we have a single grab sample per collection, when $w \\to \\delta t$ we have continuous sampling.)\nEach sample is sequenced to a total depth of $n$ reads.\n\nWe also consider the a delay of $t_d$ between the collection of the sample and the data processing.\nThis delay accounts for sample transport, sample prep, sequencing, and data analysis.\n\n## Read counts\n\nWe considered three different models of the number of reads in each sample from the epidemic virus:\n\n1. A deterministic model where the number of reads in a sample at time t is\n   $$\n   k = \\mu = n b \\int_{t-w}^{t} p(t) \\frac{dt}{w},\n   $$\n   where $b$ is the P2RA factor that converts between prevalence and relative abundance.\n2. A stochastic model that accounts for Poisson counting noise and variation in the latent relative abundance.\n   In this model, the number of reads is a random variable drawn from a Poisson-gamma mixture with mean $\\mu$ (as in 1.) and inverse overdispersion parameter $\\phi$.\n   Large $\\phi$ means that the relative abundance is well-predicted by our deterministic model, whereas small $\\phi$ means that there is a lot of excess variation beyond what comes automatically from having a finite read depth.\n3. A stochastic model where we sequence a pooled sample of $n_p$ individuals.\n   This allows us to consider the effect of sampling a small number of, e.g., nasal swabs rather than wastewater.\n\nSee\n[NAO Cost Estimate -- Adding noise](../2024-02-22_StochasticMVP/index.qmd)\nfor stochastic models.\n\n## Detection\n\nWe model detection based on the cumulative number of viral reads over all samples.\nWhen this number reaches a threshold value $\\hat{K}$, the virus is detected.\n\n## Costs\n\nWe considered two components of cost:\n\n1. The per-read cost of sequencing $d_r$\n2. The per-sample cost of collection and processing $d_s$\n\nSee [NAO Cost Estimate -- Optimizing the sampling interval](../2024-02-08_OptimalSamplingInterval/index.qmd)\nfor details.\n\n# Key results\n\n## Sequencing effort required in a deterministic model\n\nIn [NAO Cost Estimate MVP](../2024-02-02_CostEstimateMVP/index.qmd), we found the sampled depth per unit time required to detect a virus by the time it reaches cumulative incidence $\\hat{c}$ to be:\n$$\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac\n        {e^{-r\\delta t} {\\left(e^{r \\delta t} - 1\\right)}^2}\n        {{\\left(r \\delta t\\right)}^2}\n        \\right)\n    e^{r t_d}.\n$$\nThis result is for grab sampling, which in our model is a good approximation for windowed-composite sampling when $r w \\ll 1$.\n\nThe first two terms on the right-hand side are equivalent to the result from the [P2RA model](https://docs.google.com/document/d/1aHboNUDrXWAmTmZGg9Ne8dyTUdEpsluafB48DJp1UN4) using the conversion between prevalence and incidence implied by our exponential growth model.\n\nThe third term in parentheses is an adustment factor for collecting samples at $\\delta t$ intervals.\nIt includes two factors:\n\n1. the delay between when the virus is theoretically detectable and the next sample taken, and\n2. the benefit of taking a grab sample late in the sampling interval when the prevalence is higher.\n\nThis term has Taylor expansion $1 + \\frac{{(r \\delta t)}^2}{12} + \\mathcal{O}{(r\\delta t)}^3$.\n\nThe final term is the cost of the $t_d$ delay between sampling and data processing.\n\n## Optimal sampling interval\n\nIn [NAO Cost Estimate -- Optimizing the sampling interval](../2024-02-08_OptimalSamplingInterval/index.qmd),\nwe found the sampling interval $\\delta t$ that minimized the total cost.\nLonger $\\delta t$ between samples saves money on sample processing, but requires more depth to make up for the delay of waiting for the next sample after the virus becomes detectable.\nWe found that the optimal $\\delta t$ satisfies (again for grab sampling):\n\n$$\nr \\delta t \\approx {\\left(\n    6 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{k}}\n    \\left( \\frac{r}{r + \\beta} \\right)\n    e^{- r t_d}\n    \\right)}^{1/3}.\n$$\n\nWhen sampling optimally, the per-sample sequencing cost ($n d_r$) should be a multiple of the sample costs($d_s$):\n$$\nn d_r \\approx \\frac{6}{{\\left(r \\delta t\\right)}^2} d_s\n$$\n\n## Additional sequencing required to ensure a high probability of detection\n\nIn [NAO Cost Estimate -- Adding noise](../2024-02-22_StochasticMVP/index.qmd),\nwe change our detection criterion from requiring the expected number of reads to reach the threshold $\\hat{K}$\nto requiring that the number of reads reach $\\hat{K}$ with high probability, $p$.\nWe ask how much higher the cumulative incidence has to be to meet the second criterion than the first.\n\nWe find that a key parameter is $\\nu = \\frac{\\phi}{r \\delta t}$, which measures the departure of the read count distribution from Poisson.\nWhen $\\mu / \\nu \\ll 1$, the Poisson noise dominates and our detection criterion is:\n$$\n\\hat{K} \\approx \\mu + \\mu^{1/2} \\Phi^{-1}(1 - p)\n$$\nwhere $\\Phi^{-1}$ is the inverse CDF of a standard Gaussian distribution.\nSolving this equation for $\\mu$ gives the corresponding number of copies in the deterministic model required to detect with probability $p$.\n\nWhen $\\mu / \\nu \\gg 1$, the Poisson noise is small compared to the variation in the latent relative abundance.\nHere the detection criterion is:\n$$\n\\hat{K} \\approx \\mu \\left(1 + \\frac{1}{{(2\\nu)}^{1/2}} w_p(\\nu) \\right)\n$$\nwhere $w_{1-p}(\\nu) < 0$ is a function that measures the departure of the distribution from Gaussian at quantile $1-p$.\nThe term in parentheses is thus less than one and measures the ratio of the detection threshold to the deterministic approximation at detection.\n\nNumerical exploration of these regimes suggests that we expect to need 1.5--3 times more sequencing than the deterministic model predicts to detect with 95% probability by the target cumulative incidence.\n\n## Small pool noise\n\nIn the [Appendix](../2024-02-22_StochasticMVP/index.qmd#appendix-small-pool-noise) to the noise post,\nwe showed that the effect of pooling a small number of samples is controlled by $a$, the average number of viral reads each infected person contributes to the sample.\nWith fixed sampling depth, $a$ is inversely proportional to the pool size $n_p$.\nWe found that if the detection threshold is one read $\\hat{K} = 1$, sequencing depth required to ensure a given probability of detection increases in proportion to\n$$\n\\frac{a}{1 - e^{-a}}.\n$$\nWe expect a similar result to hold for higher detection thresholds.\n\n# Discussion\n\n- Nothing in our analysis here changes the intuition that the P2RA factor (here $b$) is very important for cost, especially because it appears to vary over several orders of magnitude for different viruses and studies.\n- The sampling interval is not expected to be very important for cost, assuming $r \\delta t < 1$.\n  The cost of delay from longer interval is partially offset by the benefit of sampling later when the prevalence is higher.\n- In constrast, the delay between sample collection and data analysis could matter a lot because it does not have a corresponding benefit. The required depth grows exponentially with $r t_d$.\n- We have sometimes considered the benefit to noise in the read count distribution.\n  Noisier distributions sometimes let us detect something while it is still too rare to detect on average.\n  However, our analysis here shows that if our goal is to detect by the target cumulative incidence with high probability, noise is unambiguously bad and could increase our required depth several times over.\n- We currently do not have any estimate of $\\phi$, the inverse overdispersion of read counts relative to Poisson. We should try to measure it empirically in our sequence data.\n\n## Potential extensions\n\n- We could turn this analysis into a \"plausibility map\": Given a system design (budget or $n$, $\\delta t$, $t_d$, etc.), what ranges of growth rates and P2RA factors could we detect reliably by a target cumulative incidence?\n- We could extend the model to consider multiple sampling sites.\n- The current epidemic model is completely deterministic. It would be good to check whether adding randomness changes our conclusions. (I suspect it won't in a single-population model, but may matter for multiple sampling sites.)\n- We could consider a more sophisticated detection model than just cumulative reads.\n  For example we could analyze a toy model of EGD.\n- We could explore the noise distribution of real data and try to measure $\\phi$ and whether the latent noise is mostly independent or correlated between samples.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}