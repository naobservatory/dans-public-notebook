[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Informal writeups by Dan Rice of his work on the Nucleic Acid Observatory. The intended audience is typically other NAO team members, especially future Dan. Expect incomplete posts, insufficient background, broken links to non-public documents, etc. For polished, accessible writeups of NAO work, see our blog."
  },
  {
    "objectID": "posts/2025-07-03_swab-ww-comparison/index.html",
    "href": "posts/2025-07-03_swab-ww-comparison/index.html",
    "title": "Simulating the sensitivity of swabs and wastewater",
    "section": "",
    "text": "Recently, we used our paired wastewater and swab samples to jointly estimate the distribution of viral reads we expect to see in a sample of either type when a virus is at a given prevalence in the sampled population. Here, we use the fitted model to estimate the cumulative incidence at detection for both sample types using the following approach:\nThis is meant as a quick first pass at the problem, not a definitive account of the merits of swabs vs. wastewater."
  },
  {
    "objectID": "posts/2025-07-03_swab-ww-comparison/index.html#epidemic",
    "href": "posts/2025-07-03_swab-ww-comparison/index.html#epidemic",
    "title": "Simulating the sensitivity of swabs and wastewater",
    "section": "Epidemic",
    "text": "Epidemic\nWe assume an epidemic that grows (deterministically) exponentially starting from one infected person in a population of \\(N = 2.5\\text{M}\\) people (roughly the DITP sewershed). Let incidence \\(i(t) = e^{r t} / N, t \\geq 0\\), be the fraction of the population newly infected on day \\(t\\). We also assume that infected people shed the virus at a constant rate for a fixed number of days, starting with the day they are infected. We define (shedding) prevalence, \\(p\\) to be the fraction of people shedding so that \\(p(t) = \\sum_{t' = t - d + 1}^t i(t')\\). We stop the simulation when \\(\\approx 50\\%\\) of people have been infected.\n\n# All times in days\ndoubling_time &lt;- 4\ngrowth_rate &lt;- log(2) / doubling_time\nshedding_duration &lt;- 7\n\npopulation_size &lt;- 2.5e6\n# Grow until ~50% of people have been infected.\n# With growth rate r, cumulative incidence is approximately (1/r) exp(r t).\n# CI \\approx N / 2 =&gt; t \\approx log(N r / 2) / r\nmax_day &lt;- ceiling(log(growth_rate * population_size / 2) / growth_rate)\n\nepi_model &lt;- tibble(\n  day = 0:max_day,\n  # Exponential growth from a single case\n  incidence = exp(day * growth_rate) / population_size,\n  cumulative_incidence = cumsum(incidence),\n  # Assume constant shedding for `shedding_duration` days\n  prevalence = slide_dbl(incidence, sum, .before = shedding_duration - 1, .complete = FALSE)\n)\n\nepi_model %&gt;%\n  ggplot(aes(x = day)) +\n  geom_line(aes(y = cumulative_incidence, color = \"Cumulative Incidence\")) +\n  geom_line(aes(y = prevalence, color = \"Prevalence\")) +\n  labs(x = NULL, y = NULL, color = NULL)\n\n\n\n\nEpidemic progress by day"
  },
  {
    "objectID": "posts/2025-07-03_swab-ww-comparison/index.html#sampling-and-sequencing",
    "href": "posts/2025-07-03_swab-ww-comparison/index.html#sampling-and-sequencing",
    "title": "Simulating the sensitivity of swabs and wastewater",
    "section": "Sampling and sequencing",
    "text": "Sampling and sequencing\n\nWe assume that the wastewater and pooled nasal swabs are collected every day, and that each sample is sequenced to a constant number of total reads, which differs between swabs and wastewater.\nThe fitted model predicts the number of reads that are assigned to the whole genome. Here, we are interested in the number of reads that overlap the junction. To predict the number of junction-covering reads, we assume that each read assigned to the genome has an equal probability of overlapping the junction. In our model, this thinning process is equivalent to multiplying the mean number of reads by this probability. For each sample type, we assume that all reads are the same length and that expected coverage is even along the genome. To cover a junction, we require a read to overlap the junction point by a certain number of bases. With these assumptions, probability of covering the junction is equal to the read length minus the required overlap divided by the genome length.\nWe assume that a junction is detected when reads covering it are observed in two different samples.\n\n\n# TODO: implement sampling interval. Currently implicitly daily\n# sampling_interval &lt;- 1\n\nswab_pool_size &lt;- 300\ntotal_reads_swab &lt;- 2e6\ntotal_reads_ww &lt;- 3e9\n\n# All lengths in bp\nread_length_swab &lt;- 2000\nread_length_ww &lt;- 170\n# To observe a junction, require 15bp on either side\noverlap_required &lt;- 30\n\ntaxa &lt;- tribble(\n   ~species, ~taxid, ~genome_length,\n    \"Rhinovirus A\", 147711L, 7200L,\n    \"Rhinovirus B\", 147712L, 7200L,\n    \"Rhinovirus C\", 463676L, 7100L,\n    \"HCoV-229E\", 11137L, 27500L,\n    \"HCoV-OC43\", 31631L, 30000L,\n    \"HCoV-NL63\", 277944L, 27500L,\n    \"HCoV-HKU1\", 290028L, 30000L,\n    \"SARS-CoV-2\", 2697049L, 30000L\n  ) %&gt;%\n  mutate(\n    # Probability that a read covers the junction, assuming even coverage\n    pr_junction_swab = (read_length_swab - overlap_required) / genome_length,\n    pr_junction_ww = (read_length_ww - overlap_required)/ genome_length,\n  )\n\n\n# Number of samples required to detect a junction\ndetection_samples &lt;- 2"
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html",
    "href": "posts/2024-02-22_StochasticMVP/index.html",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "",
    "text": "Previously, we have worked out a deterministic model of the sequencing depth required to detect a novel pandemic virus by the time it reaches a fixed cumulative incidence (number of people ever infected). However, in the real world, there are a number of sources of noise that will affect our ability to detect a virus. In this post, we ask the question: For a given level of sequencing, what is the cumulative incidence by which we have an x% chance of detecting a virus? We can use this result to modify our deterministic estimates of the level of sequencing required to detect by a given cumulative incidence."
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html#background",
    "href": "posts/2024-02-22_StochasticMVP/index.html#background",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "",
    "text": "Previously, we have worked out a deterministic model of the sequencing depth required to detect a novel pandemic virus by the time it reaches a fixed cumulative incidence (number of people ever infected). However, in the real world, there are a number of sources of noise that will affect our ability to detect a virus. In this post, we ask the question: For a given level of sequencing, what is the cumulative incidence by which we have an x% chance of detecting a virus? We can use this result to modify our deterministic estimates of the level of sequencing required to detect by a given cumulative incidence."
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html#model",
    "href": "posts/2024-02-22_StochasticMVP/index.html#model",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Model",
    "text": "Model\nIt is useful to categorize noise sources by how they behave as various parameters like the sequencing depth change. For example, three types of noise that are relevant to our problem are:\n\nNoise whose coefficient of variation decreases as the sequencing depth increases. This includes poisson counting noise in the number of reads mapping to a sequence, due to finite sampling effects. Variation in the number of reads per sample likely takes this form as well.\nNoise whose coefficient of variation goes to a constant value as the sequencing depth increases. For example, random variation in the efficiency of the target will contribute this type of noise. Also, the relative abundances in a sequencing library depend on biases in enrichment efficiency. If there is a class of abundant sequences that are efficiently enriched by our lab protocol, random variation in the abundance of that class of sequences will generate noise in the counts of all the other sequences that is only weakly dependent on the total read depth.\nNoise that depends on the number of people contributing to a sample. For example, in the limit where each sample is taken from a single person, the noise in the counts of reads mapping to the pandemic virus will be dominated by whether that single person is infected or not.\n\nIn the following, we consider noise classes 1 and 2. We neglect 3 for the moment because in well-mixed municipal wastewater samples, we expect this to be a small effect. However, similar analysis to that presented here could be applied in that case as well. (Update: see Appendix for update with a treatment of 3).\nWe will consider a sequence of samples indexed by \\(i\\), where the random variable \\(Y_i\\) represents the number of reads corresponding to the pandemic virus in sample \\(i\\). We model \\(Y_i\\) as independent draws from a Poisson mixture distribution: \\[\nY_i \\sim \\text{Poisson}(X_i),\n\\] where \\(X_i\\) is a latent variable that represents excess noise not accounted for by the Poisson model. To connect this model to our previous deterministic model, we set the mean of \\(X_i\\) to the number of reads in the deterministic model:\n\\[\nE[X_i] = \\mu_i = \\frac{n b}{N} e^{r(t_0 + i \\delta t)}\n\\]\nwhere:\n\n\\(n\\) is the sequencing depth\n\\(b\\) is the P2RA factor\n\\(N\\) is the population size\n\\(r\\) is the growth rate of the virus\n\\(t_0\\) is the time of the first sample after the start of the pandemic\n\\(\\delta t\\) is the time between samples.\n\nNote that for simplicity this is assuming instantaneous grab sampling, which is a good approximation to 24-hr composite sampling.\nRecall that in our detection model, we declare a virus to be detected when the cumulative number of reads matching the virus cross a threshold. Thus, to calculate the probability of detection, we need to calculate the probability that the cumulative number of reads (\\(\\sum_{j=0}^{i} Y_j\\)) is greater than the threshold value \\(\\hat{K}\\). We will proceed in two steps:\n\nCalculate the cumulant generating function (CGF) of the random variable \\(Y = \\sum_j Y_j\\). It is convenient to work with the CGF because the CGF of a sum of independent random variables is the sum of their individual CGFs.\nApproximate the cumulative distribution function (CDF) of \\(Y\\) from a finite set of cumulants using the Cornish-Fisher expansion. In this notebook, we will explore under what conditions we can truncate the Cornish-Fisher expansion at a certain number of terms."
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html#cumulant-generating-function-of-the-cumulative-read-count-y",
    "href": "posts/2024-02-22_StochasticMVP/index.html#cumulant-generating-function-of-the-cumulative-read-count-y",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Cumulant generating function of the cumulative read count, \\(Y\\)",
    "text": "Cumulant generating function of the cumulative read count, \\(Y\\)\nThe cumulant generating function \\(K_Y\\) of random variable \\(Y\\) is given by the log of its moment generating function:\n\\[\nK_Y(z) = \\log \\mathbb{E}[e^{zY}].\n\\]\nIf \\(Y_i\\) is Poisson distributed with random mean \\(X_i\\),\n\\[\n\\begin{align}\nK_{Y_i}(z) & = \\log \\mathbb{E}\\left[\\mathbb{E}[e^{zY_{i}} | X_i]\\right] \\\\\n       & = \\log \\mathbb{E}\\left[\\exp \\left\\{ X_i (e^{z} - 1) \\right\\} \\right] \\\\\n       & = K_{X_i} \\left(e^{z} - 1\\right),\n\\end{align}\n\\] where the second line uses the moment-generating fuction of a Poisson random variable, and \\(K_{X_i}\\) is the CGF of \\(X_i\\).\nIf we assume that the \\(Y_i\\) are independent of one another, then we can add their CGFs to get the CGF of the cumulative read count: \\[\n\\begin{align}\nK_Y(z) & = K_{\\sum_i Y_i}(z) \\\\\n       & = \\sum_i K_{Y_i}(z) \\\\\n       & = \\sum_i K_{X_i}(e^z - 1) \\\\\n       & = K_{X}(e^z - 1),\n\\end{align}\n\\] where we define \\(X \\equiv \\sum_i X_i\\).\nThe last equation tells us how to combine the cumulants of \\(X\\) to get the cumulants of \\(Y\\). Let the cumulants of \\(Y\\) be denoted \\(\\kappa_1, \\kappa_2, \\ldots\\) and the cumulants of \\(X\\) by \\(\\chi_1, \\chi_2, \\ldots\\). Expanding the CGF gives: \\[\n\\begin{align}\nK_Y(z) & = K_X(e^z - 1) \\\\\n       & = \\chi_1 (e^z - 1) + \\chi_2 \\frac{{(e^z-1)}^2}{2} + \\chi_3 \\frac{{(e^z-1)}^3}{3!} + \\cdots \\\\\n       & = \\chi_1 \\sum_{j=1}^{\\infty} \\frac{z^j}{j!} + \\chi_2 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^2}{2} + \\chi_3 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^3}{3!} + \\cdots \\\\\n\\end{align}\n\\] Then, by equating powers of \\(z\\), we find \\[\n\\begin{align}\n\\kappa_1 & = \\chi_1 \\\\\n\\kappa_2 & = \\chi_1 + \\chi_2 \\\\\n\\kappa_3 & = \\chi_1 + 3 \\chi_2 + \\chi_3 \\\\\n\\kappa_4 & = \\chi_1 + 7 \\chi_2 + 6 \\chi_3 + \\chi_4 \\\\\n         & \\cdots\n\\end{align}\n\\] This cumulant series has the intuitive property that if \\(X \\to \\lambda\\) constant, \\(\\chi_\\alpha \\to \\lambda \\delta_{\\alpha, 1}\\) and \\(\\kappa_\\alpha \\to \\chi_1\\). That is, \\(Y \\to \\text{Poisson}(\\lambda)\\). For random \\(X\\), in constrast, all of the cumulants of \\(Y\\) are increased from their Poisson value of \\(\\chi_1\\) by the cumulants of \\(X\\). In particular, the variance of \\(Y\\), \\(\\kappa_2\\) is equal to the Poisson variance \\(\\chi_1\\) plus the variance of \\(X\\), \\(\\chi_2\\).\n\nCumulants of the latent variable \\(X\\)\nIt remains to find the cumulants of \\(X\\), \\(\\chi_\\alpha\\). For this, we need to specify a distribution for the latent variables at each sample, \\(X_i\\). For simplicity, we will choose the Gamma distribution, which allows us to vary the mean and coefficient of variation independently. We will parameterize the distribution by its mean \\(\\mu\\), and inverse dispersion \\(\\phi\\). In standard shape-scale parameterization, we have: \\[\nX_i \\sim \\text{Gamma}(\\phi, \\mu_i / \\phi)\n\\] where we assume that the inverse dispersion is constant in time. Note that the coefficient of variation of \\(X_i\\) is \\(\\phi^{-1/2}\\), independent of \\(\\mu_i\\). Thus our latent gamma model accounts for noise type 2 above.\nThe gamma distribution has CGF: \\[\n\\begin{align}\nK_{X_i}(z) & = - \\phi \\log(1 - \\frac{\\mu_i}{\\phi} z) \\\\\n           & = \\phi \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha}\n\\end{align}\n\\]\nBy the summation property of CGFs, we have the CGF of \\(X = \\sum_j X_j\\) at time \\(t_i\\): \\[\n\\begin{align}\nK_{X}(z) & = \\sum_{j=0}^i K_{X_j}(z) \\\\\n         & = \\phi \\sum_{j=0}^i \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha} \\\\\n         & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left( \\sum_{j=0}^{i} \\mu_j^\\alpha \\right) z^\\alpha.\n\\end{align}\n\\] Because the prevalence is growing exponentially in our model, \\(\\mu_j\\) is growing exponentially (see above). Letting \\(A \\equiv \\frac{nb}{N} e^{rt_0}\\), we have \\(\\mu_j = A e^{rj\\delta t}\\) and thus\n\\[\n\\begin{align}\nK_X(z) & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left(\\sum_{j=0}^{i} A^\\alpha e^{\\alpha r j \\delta t} \\right) z^\\alpha \\\\\n       & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} A^\\alpha \\left(\\frac{e^{\\alpha r (i+1) \\delta t} - 1}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha \\left(\\frac{\\alpha r \\delta t e^{\\alpha r \\delta t}}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha z^\\alpha\n\\end{align}\n\\] Where the first approximation requires the prevalence to be large compared to one, and the second approximation requires \\(\\alpha r \\delta t \\ll 1\\). (TODO: generalize the second approximation.)\nIt is illuminating to parameterize the distribution of \\(X\\) by its mean, \\(\\mu\\), and a shape parameter \\(\\nu\\): \\[\n\\begin{align}\n\\mu & \\equiv K_X'(0) \\\\\n    & = \\frac{A e^{r i \\delta t}}{r \\delta t}. \\\\\n\\nu & \\equiv \\frac{\\phi}{r \\delta t}.\n\\end{align}\n\\] Substituting these into the equation above gives \\[\nK_X(z) = \\nu \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha^2} {\\left(\\frac{\\mu}{\\nu}\\right)}^\\alpha z^\\alpha.\n\\] (Note that this is similar to the CGF of the gamma distribution but with the logarithm replaced by a dilogarithm.)\nFinally, examination of the CGF yields the cumulants of \\(X\\): \\[\n\\chi_\\alpha = \\frac{(\\alpha - 1)!}{\\alpha} \\nu {\\left(\\frac{\\mu}{\\nu}\\right)}^{\\alpha}.\n\\]\nWe can say a few things about this result:\n\nBy construction, the mean of \\(X\\), \\(\\mu\\), is our previous deterministic prediction for the counts. (In the small \\(r \\delta t\\) limit).\nThe shape parameter \\(\\nu\\) controls the dispersion of \\(X\\). \\(\\text{Var}[X] = \\chi_2 = \\frac{\\mu^2}{2\\nu}\\). That is: larger \\(\\nu\\) means a smaller coefficient of variation.\n\\(\\nu\\) is controled by the latent inverse dispersion \\(\\phi\\) and the scaled sampling interval \\(r \\delta t\\). Smaller sampling inverval means we take more independent samples per unit time, which reduces the variance of the sum.\n\\(\\frac{\\mu}{\\nu}\\) is a pure scale parameter of the distribution of \\(X\\).\n\n\n\nCumulants of the cumulative counts \\(Y\\)\nSubstituting our equation for the cumulants of \\(X\\) \\(\\chi_\\alpha\\) into the equation for the cumulants of \\(Y\\) above gives \\[\n\\begin{align}\n\\kappa_1 & = \\mu \\\\\n\\kappa_2 & = \\mu + \\frac{1}{2} \\frac{\\mu^2}{\\nu} \\\\\n\\kappa_3 & = \\mu + \\frac{3}{2} \\frac{\\mu^2}{\\nu} + \\frac{2}{3} \\frac{\\mu^3}{\\nu^2} \\\\\n\\kappa_4 & = \\mu + \\frac{7}{2} \\frac{\\mu^2}{\\nu} + 4 \\frac{\\mu^3}{\\nu^2} + \\frac{3}{2} \\frac{\\mu^4}{\\nu^3}. \\\\\n\\end{align}\n\\]\nWe have two regimes, controled by the parameter \\(\\mu / \\nu\\):\n\nIf \\(\\frac{\\mu}{\\nu} \\ll 1\\), the Poisson noise dominates and \\(\\kappa_\\alpha \\approx \\mu\\).\nIf \\(\\frac{\\mu}{\\nu} \\gg 1\\), the latent noise dominates and \\(\\kappa_\\alpha \\approx \\chi_\\alpha\\).\n\nFor higher cumulants, the separation between the regimes becomes less clean (i.e. it takes a smaller or larger \\(\\mu/\\nu\\) for one term to dominate.)\nIn terms of model parameters:\n\nMore frequent samples (smaller \\(\\delta t\\), thus larger \\(\\nu\\)) pushes us toward the Poisson-dominant regime.\nMore variable latent abundances (smaller \\(\\phi\\), thus smaller \\(\\nu\\)) pushes us toward the latent-dominant regime.\nA higher threshold of detection (larger \\(\\mu\\) at the time of detection) pushes us toward the latent-dominant regime."
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html#the-cornish-fisher-expansion-of-the-quantiles-of-y",
    "href": "posts/2024-02-22_StochasticMVP/index.html#the-cornish-fisher-expansion-of-the-quantiles-of-y",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "The Cornish-Fisher expansion of the quantiles of \\(Y\\)",
    "text": "The Cornish-Fisher expansion of the quantiles of \\(Y\\)\nUltimately, our goal is to estimate the probability that \\(Y &gt; \\hat{K}\\), the detection threshold. Thus, we need to estimate the CDF of \\(Y\\) from its cumulants \\(\\kappa_\\alpha\\). For that we will use the Cornish-Fisher expansion. The idea behind the expansion is to start by approximating the CDF as that of a Gaussian random variable with the correct mean and variance, and then iteratively adjust it for higher-order cumulants (skew, kurtosis, etc). It is defined as follows:\nThe quantile function \\(y(p)\\) (i.e. the value for which \\(\\text{CDF}(y) = p\\)) is approximated by \\(y(p) \\approx \\kappa_1 + {\\kappa_2}^{1/2} w_p\\), where \\[\n\\begin{align}\nw_p & = x \\\\\n    & + \\gamma_1 h_1(x) \\\\\n    & + \\gamma_2 h_2(x) + {\\gamma_1}^2 h_{11}(x) \\\\\n    & + \\cdots \\\\\nx   & = \\Phi^{-1}(p) \\\\\n\\gamma_{\\alpha-2} & = \\frac{\\kappa_\\alpha}{{\\kappa_2}^{\\alpha/2}} \\\\\nh_1(x) & = \\frac{\\text{He}_2(x)}{6} \\\\\nh_2(x) & = \\frac{\\text{He}_3(x)}{24} \\\\\nh_{11}(x) & = - \\frac{2\\text{He}_3(x) + \\text{He}_1(x)}{36} \\\\\n\\end{align}\n\\] Where \\(\\Phi\\) is the CGF of the standard normal distribution and \\(\\text{He}\\) are the probabilists’ Hermite polynomials. Note that each line of the sum must be included as a whole for the approximation to be valid at that level.\n\nValidity of the expansion\nFor fixed \\(x\\) (and therefore fixed quantile), the relative sizes of the terms of the expansion are controlled by the coefficients \\(\\gamma\\). In the Poisson-dominated regime: \\[\n\\begin{align}\n\\gamma_1 & = \\mu^{-1/2} \\\\\n\\gamma_2 & = \\gamma_1^2 = \\mu^{-1} \\\\\n\\end{align}\n\\] so truncating the expansion at a few terms should work well when \\(\\mu &gt; 1\\). Since we’re interested in having a significant probability of detection (e.g. &gt;90%), and our threshold of detection is at least one read, this is not a very limiting requirement.\nIn the latent-noise-dominated regime: \\[\n\\begin{align}\n\\gamma_1 & = \\frac{2^{5/2}}{3} \\nu^{-1/2} \\\\\n\\gamma_2 & = 6 \\nu^{-1}\n\\end{align}\n\\] so truncating the series at a few terms should work well when \\(\\nu &gt; 1\\). This is also not very limiting, because \\(\\nu\\) can be large either by making \\(\\phi\\) large (which is likely unless our data source is extremely noisy) or by sampling frequently so that \\(r \\delta t\\) is small.\nPutting these together, we can see that for our purposes, the expansion will be good when \\(\\nu &gt; 1\\). We’re concerned with the value of \\(\\mu\\) when a particular quantile (e.g. 5%) passes the detection threshold \\(\\hat{K}\\) The detection threshold by definition must be at least one read. Therefore, at detection, \\(\\mu &gt; 1\\). If \\(\\mu &lt; \\nu\\), the Poisson noise dominates we can use \\(\\mu &gt; 1\\) to establish the validity of the CF expansion. If \\(\\mu &gt; \\nu\\), the latent noise dominates and we can use \\(\\nu &gt; 1\\) for validity."
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html#numerical-calculations",
    "href": "posts/2024-02-22_StochasticMVP/index.html#numerical-calculations",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Numerical calculations",
    "text": "Numerical calculations\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial.polynomial import Polynomial\nfrom numpy.polynomial.hermite_e import HermiteE\nfrom scipy.special import factorial\nfrom scipy.stats import poisson, norm, gamma\n\n\ndef cgf_x_series(mu: float, nu: float, order: int = 4) -&gt; Polynomial:\n    coeffs = np.zeros(order + 1)\n    # No zeroth coefficient\n    k = np.arange(1, order + 1)\n    coeffs[1:] = nu * (mu / nu) ** k / k**2\n    return Polynomial(coeffs)\n\n\ndef expm1_series(order: int = 4) -&gt; Polynomial:\n    k = np.arange(order + 1)\n    return Polynomial(1.0 / factorial(k)) - 1\n\n\ndef cgf_y_series(mu: float, nu: float, order: int = 4):\n    return cgf_x_series(mu, nu, order)(expm1_series(order)).cutdeg(order)\n\n\ndef cumulant_from_cgf(cgf: Polynomial, order: int) -&gt; float:\n    return cgf.deriv(order)(0)\n\nSpot-check cumulants:\n\nmu = 2.0\nnu = 10.0\ncgf = cgf_x_series(mu, nu)(expm1_series()).cutdeg(4)\ncoeffs = [0, 1, (1, 1 / 2), (1, 3 / 2, 2 / 3), (1, 7 / 2, 4, 3 / 2)]\npredicted_cumulants = [mu * Polynomial(c)(mu / nu) for c in coeffs]\nfor k in range(5):\n    print(cumulant_from_cgf(cgf, k), predicted_cumulants[k])\n\n0.0 0.0\n2.0 2.0\n2.2 2.2\n2.6533333333333333 2.6533333333333333\n3.7439999999999998 3.744\n\n\nCheck variance:\n\nnu = 10.0\nmu = np.arange(1, 100)\nk2 = [cumulant_from_cgf(cgf_y_series(m, nu), 2) for m in mu]\nplt.plot(mu, k2, label=\"exact\")\nplt.plot(mu, (1 / 2) * mu**2 / nu, \"--\", label=\"latent approximation\")\nplt.xlabel(r\"$\\mu$\")\nplt.ylabel(r\"Variance of $Y$\")\nplt.legend()\n\n\n\n\n\n\n\n\nNote that the variance is growing quadratically, but the Poisson contribution to the variance is not totally negligible even for large \\(\\nu\\).\n\nChecking the Cornish-Fisher expansion against common distributions\nIn this section, we compare the Cornish-Fisher expansion of the quantile function to the exact quantile function for several known distributions to get a sense of its accuracy at different orders.\n\ndef cornish_fisher(*cumulants):\n    # cumulants = (k_1, k_2, ...)\n    order = len(cumulants)\n    if order &lt; 2:\n        raise ValueError(\"Order of approximation must be &gt;= 2\")\n    if order &gt; 4:\n        raise ValueError(\"Order of approximation must be &lt;= 4\")\n    sigma = np.sqrt(cumulants[1])\n    poly = HermiteE((0, 1))\n    if order &gt;= 3:\n        gamma_1 = cumulants[2] / sigma**3\n        h_1 = HermiteE((0, 0, 1)) / 6\n        poly += gamma_1 * h_1\n    if order &gt;= 4:\n        gamma_2 = cumulants[3] / sigma**4\n        h_2 = HermiteE((0, 0, 0, 1)) / 24\n        h_11 = -HermiteE((0, 1, 0, 2)) / 36\n        poly += gamma_2 * h_2 + gamma_1**2 * h_11\n    return cumulants[0] + sigma * poly\n\nCheck against Poisson distribution:\n\norder = 4\np = np.arange(0.01, 1.0, 0.01)\nx = norm.ppf(p)\n\nfor lamb in [1, 2, 4, 8]:\n    poisson_cumulants = [lamb] * order\n    for o in range(2, order + 1):\n        cf_poisson = cornish_fisher(*poisson_cumulants[:o])\n        plt.plot(p, cf_poisson(x), label=o)\n    plt.plot(p, poisson(lamb).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\")\n    plt.legend(title=\"order\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(f\"$\\lambda$ = {lamb}\")\n    plt.show()\n\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8544/3927045381.py:14: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f\"$\\lambda$ = {lamb}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Poisson case, when \\(\\lambda &gt; 1\\), the distribution converges quickly to a Gaussian, and the higher-order corrections don’t matter.\nCheck against Gamma distribution:\n\nscale = 1\nk = np.arange(1, order + 1)\n\nfor shape in [1 / 2, 1, 2, 4, 8]:\n    gamma_cumulants = factorial(k - 1) * shape * scale**k\n    for o in range(2, order + 1):\n        cf_gamma = cornish_fisher(*gamma_cumulants[:o])\n        plt.plot(p, cf_gamma(x), label=o)\n    plt.plot(\n        p, gamma(shape, scale=scale).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\"\n    )\n    plt.legend(title=\"order\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(f\"shape = {shape}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape parameter controls deviations from Gaussian and the error of the higher order approximations. For shape &gt; 1, three terms gets you close and 4 gets you very close. For shape = 1/2, the Gaussian approximation is quite bad, but the order-4 Cornish fisher approximation is decent except for small quantiles. (It does not capture the power-law left tail of the distribution.)\n\n\nCornish-Fisher expansions for the cumulative count distribution\nFor the cumulative count distribution, there are two parameters, \\(\\mu\\) (the mean) and \\(\\nu\\) (which determines the shape).\nFor small \\(\\nu\\), the noise is quickly dominated by the latent variable so the distribution goes to a constant shape, scaled by \\(\\mu\\).\nEven with \\(\\nu = 2\\), the CF expansion converges quickly. 3 terms is quite good, 4 is indistinguisable from higher:\n\norder = 6\nnu = 2.0\nfor mu in [1, 2, 4, 8, 32]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend(title=\"order\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith larger \\(\\nu\\), the shape of the distribution is closer to Gaussian:\n\norder = 6\nnu = 10.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith \\(\\nu = 100\\), there is a wide Poisson-dominated regime the convergence to Gaussian is quite fast.\n\norder = 4\nnu = 100.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.xlabel(\"quantile\")\n    plt.ylabel(\"value\")\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPercentiles\nThe following show the mean (dashed grey lines) and the 10th percentile (black) of cumulative counts as a function of the mean \\(\\mu\\). Colored lines are the latent and poisson regime approximations. Each plot has a different shape parameter \\(\\nu\\).\n\norder = 4\nks = range(1, order + 1)\n# 10th percentile\np = 0.1\nx = norm.ppf(p)\n\nmus = np.arange(1, 100)\nfor nu in [4, 10, 100]:\n    cgfs = [cgf_y_series(mu, nu) for mu in mus]\n    cumulants = [[cumulant_from_cgf(cgf, k) for k in ks] for cgf in cgfs]\n    cumulants_latent = [\n        [factorial(k - 1) / k * nu * (mu / nu) ** k for k in ks] for mu in mus\n    ]\n    cumulants_poisson = [[mu for _ in ks] for mu in mus]\n    cf = [cornish_fisher(*cs)(x) for cs in cumulants]\n    cf_l = [cornish_fisher(*cs)(x) for cs in cumulants_latent]\n    cf_p = [cornish_fisher(*cs)(x) for cs in cumulants_poisson]\n    plt.plot(mus, cf, \"k\", label=\"full\")\n    plt.plot(mus, cf_l, color=\"C0\", label=\"latent\")\n    plt.plot(mus, cf_p, color=\"C1\", label=\"poisson\")\n    plt.plot(mus, mus, \"--\", color=\"grey\", label=\"mean\")\n    plt.xlabel(r\"$\\mu$\")\n    plt.ylabel(\"Cumulative counts\")\n    plt.title(r\"$\\nu = $\" f\"{nu}\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nSmaller \\(\\nu\\) means that we have a greater reduction of the 10th percentile from the mean.\nSmaller \\(\\nu\\) means that the 10th percentile increase linearly (as expected in the latent regime).\nAs \\(\\nu\\) gets larger, there is a wider Poisson-dominated regime where the tenth percentile increases like \\(\\mu + \\mu^{1/2} const.\\).\nUnless \\(\\nu\\) is very large, we can probably get away with the latent-regime approximation"
  },
  {
    "objectID": "posts/2024-02-22_StochasticMVP/index.html#implications-for-cost",
    "href": "posts/2024-02-22_StochasticMVP/index.html#implications-for-cost",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Implications for cost",
    "text": "Implications for cost\nIn the Poisson-dominated regime, a Gaussian approximation is pretty good, so \\[\ny(p) \\approx \\mu + \\mu^{1/2} \\Phi^{-1}(p)\n\\] Note that this is true even when the detection threshold \\(\\hat{K} = 1\\) because the mean will have to be larger than one to have a high probability of detection. We can let \\(p\\) be one minus the target probability of detection, set \\(y(p) = \\hat{K}\\), and solve for \\(\\mu\\). This allows us to calculate the delay in detection due to having to wait for the mean to be larger than the threshold.\nThe Poisson regime will be more appropriate for small read counts and thus small thresholds. Let’s consider the effect of stochasticity on detection when the threshold \\(\\hat{K}\\) is low. If we detect when we observe two reads, the previous equation shows that we will need \\(\\mu \\geq 4.8\\) to detect 10% of the time. In contrast, our deterministic model predicts that we detect when \\(\\mu = 2\\). Thus, the Poisson noise costs us over a full doubling time in detection sensitivity.\n\nfrom scipy.optimize import fsolve\n\n\ndef mu_at_detection(k_hat, p):\n    def f(mu):\n        return mu + norm.ppf(p) * mu ** (1 / 2) - k_hat\n\n    return fsolve(f, k_hat)[0]\n\n\nprint(mu_at_detection(2.0, 0.1))\n\nk_hat = np.arange(1, 20)\nmu_10 = np.array([mu_at_detection(k, 0.1) for k in k_hat])\nmu_05 = np.array([mu_at_detection(k, 0.05) for k in k_hat])\nplt.plot(k_hat, mu_10, \".\", label=\"Pr{detect}=0.1\")\nplt.plot(k_hat, mu_05, \".\", label=\"Pr{detect}=0.05\")\nplt.plot(k_hat, k_hat, \"--k\", label=\"deterministic\")\nplt.title(\"Poisson approximation\")\nplt.legend()\nplt.ylabel(r\"$\\mu$ at detection\")\nplt.xlabel(r\"Detection threshold $\\hat{K}$\")\nplt.show()\n\nplt.plot(k_hat, mu_10 / k_hat)\nplt.plot(k_hat, mu_05 / k_hat)\nplt.ylabel(\"Inflation factor relative to deterministic\")\nplt.xlabel(r\"Detection threshold $\\hat{K}$\")\nplt.ylim([1, 4.5])\n\n4.810935246946805\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Latent-dominated regime, the terms of the Cornish-Fisher expansion just depend on \\(\\nu\\), not on \\(\\mu\\), so: \\[\n\\begin{align}\ny(p) & \\approx \\mu + \\frac{\\mu}{{(2\\nu)}^{1/2}} w_p(\\nu) \\\\\n     & = \\mu \\left(1 +\\frac{1}{{(2\\nu)}^{1/2}} w_p(\\nu)\\right) \\\\\n\\end{align}\n\\] Can calculate \\(w_p(\\nu)\\) to as high order as we need, then solve for \\(\\mu\\) as in the Poisson regime. Because \\(w_p\\) will be negative (since \\(p\\) is small), this will inflate the required \\(\\mu\\) for detection by a factor. This suggests a path for data analysis: if we can estimate \\(\\nu\\) from data (by estimating the overdispersion parameter \\(\\phi\\) in the read count distribution and then scaling by \\(r \\delta t\\), we can estimate the inflation factor.\n\ndef cf_latent(nu):\n    return HermiteE((1, (2 * nu) ** (-1 / 2), (2 / 9) / nu))\n\n\nnu = np.arange(1.0, 20.0)\nfactor_10 = np.array([cf_latent(n)(norm.ppf(0.1)) for n in nu])\nfactor_05 = np.array([cf_latent(n)(norm.ppf(0.05)) for n in nu])\nplt.plot(nu, 1 / factor_10, label=\"Pr{detect}=0.1\")\nplt.plot(nu, 1 / factor_05, label=\"Pr{detect}=0.05\")\nplt.legend()\nplt.ylabel(\"Inflation factor relative to deterministic\")\nplt.xlabel(r\"Shape parameter $\\nu$\")\nplt.ylim([1, 4.5])\n\n\n\n\n\n\n\n\nThis suggests that the deterministic approximation will be within a doubling when \\(\\nu \\gtrapprox 5\\). However, it is never very close.\nPutting these results together suggests that regardless of the regime, we expect the sequencing depth required to achieve detection probability of 0.9 or 0.95 at the target cumulative incidence to be roughly 1.5 to 3.0 times higher under a stochastic model than under the deterministic approximation.\nThe above assumes that \\(\\nu &gt; 1.5\\) or so. For smaller \\(\\nu\\), our approximations break down. However, smaller \\(\\nu\\) means more variability. Under those circumstances, the details of the distribution of sequencing outcomes likely matter in some detail."
  },
  {
    "objectID": "posts/2024-02-02_CostEstimateMVP/index.html",
    "href": "posts/2024-02-02_CostEstimateMVP/index.html",
    "title": "NAO Cost Estimate MVP",
    "section": "",
    "text": "See Google Doc for background. See also Simple detection cost model for P2RA version."
  },
  {
    "objectID": "posts/2024-02-02_CostEstimateMVP/index.html#background",
    "href": "posts/2024-02-02_CostEstimateMVP/index.html#background",
    "title": "NAO Cost Estimate MVP",
    "section": "",
    "text": "See Google Doc for background. See also Simple detection cost model for P2RA version."
  },
  {
    "objectID": "posts/2024-02-02_CostEstimateMVP/index.html#epidemic-model",
    "href": "posts/2024-02-02_CostEstimateMVP/index.html#epidemic-model",
    "title": "NAO Cost Estimate MVP",
    "section": "Epidemic model",
    "text": "Epidemic model\n\nPrevalence and incidence\nConsider a population of \\(N\\) individuals. We focus on the phase of the epidemic when it is growing exponentially. This will be approximately true at an intermediate time when the virus is common enough that we can neglect noise in reproduction, but rare enough that it is not running out of susceptible people to infect. Let the rate of new infections per infected persion per unit time be \\(N \\alpha\\) and the rate of recovery of an infected person to be \\(\\beta\\). Then, we have the following ODE for the prevalence (the number of infected people, not the proportion of the population as in the P2RA report) \\(P(t)\\):\n\\[\n\\frac{dP}{dt} \\approx \\alpha N P - \\beta P.\n\\]\nThe first term on the right-hand side is the incidence per unit time: \\(I \\approx \\alpha N P\\). (Note that we are assuming that the number of susceptible people is approximately \\(N\\), which will cease to be true as the virus becomes common.)\nSolving this equation and choosing \\(t = 0\\) to be the time when one person is infected gives: \\[\n\\begin{align}\nP(t) & = e^{(\\alpha N - \\beta) t} \\\\\n     & = e^{rt} \\\\\nI(t) & = \\alpha N e^{rt} \\\\\n     & = (r + \\beta) e^{rt},\n\\end{align}\n\\] where we have defined the exponential growth rate \\(r \\equiv \\alpha N - \\beta\\).\n\n\nCumulative incidence\nAs a yardstick for measuring the progress of the epidemic, we are interested in the cumulative incidence, \\(C(t)\\), the number of people who have ever been infected. Integrating \\(I(t)\\) with the initial condition \\(C(0) = 1\\) to account for the single individual infected at \\(t = 0\\), we have: \\[\n\\begin{align}\nC(t) & = 1 + \\int_0^t I(t') dt' \\\\\n     & = 1 + \\frac{r + \\beta}{r} (e^{rt} - 1).\n\\end{align}\n\\] Note that when sick individuals never recover (\\(\\beta = 0\\)), \\(C(t) = P(t)\\), as expected.\nA useful feature of the exponential growth regime is that all of our quantities of interest grow exponentially at the same rate and are thus in proportion to one another: \\[\n\\begin{align}\nI(t) & = (r + \\beta) P(t) \\\\\nC(t) & \\sim \\frac{r + \\beta}{r} P(t), \\text{as } t \\to \\infty.\n\\end{align}\n\\]\nRearranging, we see that the fraction of cumulative incidence that are currently infected is controled by the ratio of growth rate to recovery rate: \\[\n\\frac{P(t)}{C(t)} \\sim \\frac{r}{r + \\beta}.\n\\] This ranges from zero when the epidemic grows slowly and individuals recover quickly, to one wher the epidemic grows much faster than recovery. For a virus like SARS-CoV-2, where both doubling and recovery times were on the order of a week, we expect on the order of half of the cumulative infections to be currently sick during the exponetial phase.\n\n\nDomain of validity\nAs described above, the approximation of deterministic exponential growth is valid when the number of infected individuals is large enough that the growth is roughly deterministic but small enough that most of the population is still susceptible. Here we’ll approximate those bounds.\n\nDeterministic growth\nA simple stochastic model of the start of an epidemic is that infections and recoveries in a small increment of time \\(dt\\) are independent of one another, that is: \\[\n\\begin{align}\n\\text{\\# Births} & \\sim \\text{Poisson}(\\alpha N P dt) \\\\\n\\text{\\# Deaths} & \\sim \\text{Poisson}(\\beta P dt).\n\\end{align}\n\\] This implies that the change in prevalence, \\(dP = (\\text{\\# Births}) - (\\text{\\# Deaths})\\), during \\([t, t+dt)\\) has mean and variance: \\[\n\\begin{align}\n\\mathbb{E}(dP) & = (\\alpha N - \\beta) P dt \\\\\n               & = r P dt \\\\\n\\mathbb{V}(dP) & = (\\alpha N + \\beta) P dt \\\\\n               & = (r + 2 \\beta) P dt.\n\\end{align}\n\\]\nA deterministic approximation is good when the coefficient of variation of \\(dP\\) is much smaller than one, that is: \\[\n\\begin{align}\n\\frac{\\sqrt{\\mathbb{V}(dP)}}{\\mathbb{E}(dP)} & \\ll 1 \\\\\n\\frac{\\sqrt{(r + 2 \\beta) P dt}}{r P dt} & \\ll 1 \\\\.\n\\end{align}\n\\] Some algebra gives the condition that the prevalence is larger than a critical value: \\[\nP \\gg \\frac{r + 2 \\beta}{r^2 dt}.\n\\] It remains to choose a suitable increment of time. We expect the prevalence to change significantly (\\(\\mathbb{E}(\\frac{dP}{P}) = 1\\)) on the timescale \\(r^{-1}\\) so \\(dt = r^{-1}\\) is a reasonable choice as long as \\(r \\gtrsim \\beta\\). This gives the condition \\[\nP \\gg \\frac{r + 2 \\beta}{r} = 1 + 2 \\beta / r.\n\\] When this condition is met, we expect the epidemic to grow approximately deterministically at rate \\(r\\).\nNote that this condition is violated at \\(t = 0\\). We must thus reinterpret \\(t = 0\\) as the effective time that the epidemic would have infected a single person if our deterministic approximation for \\(P \\gg 1 + 2\\beta/r\\) were extended backwards in time. [TODO: estimate the error in cumulative infections introduced by making this approximation.]\n\n\nExponential growth\nAs the virus spreads, the number of susceptible people declines, reducing the rate of spread and resulting in sub-exponential growth. Let the number of susceptible people be \\(S(t)\\), then the incidence per time is: \\[\nI(t) = \\alpha S(t) P(t).\n\\] Above, we assumed that \\(S(t) = N\\). \\(I(t)\\) will thus be reduced by half by the time \\(S(t) = N / 2\\). This is a convenient upper bound on the domain of validity.\nAssuming that anyone previously infected is no longer susceptible, we have: \\[\n\\begin{align}\nS(t) & \\gg N / 2 \\\\\nN - C(t) & \\gg N / 2 \\\\\nC(t) & \\ll N / 2.\n\\end{align}\n\\]\nUsing \\(C = \\frac{r + \\beta}{r} P\\) in the exponential regime, we can put this together with our lower bound of the deterministic approximation to get the region of validity: \\[\n1 + 2\\beta / r \\ll P \\ll \\frac{N}{2 (1 + \\beta / r)}.\n\\] This shows that as long as the population is large (\\(N \\gg 1\\)), and the growth rate is not too slow compared to the recovery rate (\\(r \\gtrsim \\beta\\)), there will be a wide range of \\(P\\) for which growth is approximately deterministic and exponential."
  },
  {
    "objectID": "posts/2024-02-02_CostEstimateMVP/index.html#sampling-sequencing-and-detection",
    "href": "posts/2024-02-02_CostEstimateMVP/index.html#sampling-sequencing-and-detection",
    "title": "NAO Cost Estimate MVP",
    "section": "Sampling, sequencing, and detection",
    "text": "Sampling, sequencing, and detection\nConsider a set of samples taken at times \\(\\{t_0, t_1, \\ldots\\}\\), where \\(t_0 \\ge 0\\) is the the first sampling time after the virus begins to spread. (Presumably sampling has been ongoing but we’ll ignore the earlier samples.) We sequence sample \\(i\\) to a total depth of \\(n_i\\) reads and find that \\(k_i\\) match the sequence of the pandemic virus.\nWe consider the virus to be detected when the cumulative number of viral reads reaches a threshold \\(\\hat{K}\\). We’ll define \\(\\hat{t}\\) to be the smallest sample time \\(t_i\\) such that: \\[\nK_i \\equiv \\sum_{j=0}^i k_j \\geq \\hat{K}.\n\\] We also want to consider the effect of the delay, \\(t_d\\), between when the critical sample is collected and when we have processed, sequenced, and analyzed the data.\nWe will assess the success of a method by the (population-scaled) cumulative incidence at the time of detection, accounting for the delay: \\[\nc(\\hat{t} + t_d) \\equiv C(\\hat{t} + t_d) / N\n\\]\nWe’re interested in taking samples from a large population (\\(N \\gg 1\\)). In such a population, our ability to detect will depend not on the the raw number of people infected \\(P\\), but on the proportion \\(p \\equiv P / N\\). At detection, \\(p\\) will be at some typical magnitude that depends on our sensitivity but does not depend on \\(N\\). We can simplify our equations by neglecting terms that are \\(\\mathcal{O}(N^{-1})\\) while keeping terms of order \\(p = e^{r t} / N\\). This is valid because we’ve already assumed that \\(e^{r t} \\gg 1\\) so the latter are always larger than the former.\nUsing the equations above for cumulative incidence, we have the following simplification: \\[\n\\begin{align}\nc(t) & = \\frac{1 + \\frac{r + \\beta}{r}\\left(e^{rt} - 1\\right)}{N} \\\\\n     & = \\frac{r + \\beta}{r} \\frac{e^{rt}}{N} + \\mathcal{O}(N^{-1}).\n\\end{align}\n\\]\nNote that we can immediately see the cost of delay: \\[\nc(\\hat{t} + t_d) = \\frac{r + \\beta}{r} \\frac{e^{r\\hat{t}}}{N} e^{r t_d}.\n\\] With an exponentially growing pandemic, post-sampling delay multiplies our cumulative incidence at detection by \\(e^{rt_d}\\).\nIn the rest of this section, we answer the question:\n\nIf we want to detect the virus by the time it reaches a particular cumulative incidence, \\(\\hat{c}\\), how much sequencing do we need to do per unit time?\n\n\nSampling to relative abundance\nHere we make two more important deterministic assumptions:\n\nThe viral read counts are deterministic, given the expected relative abundance, \\(a_i\\), in each sample: \\(k_i = n_i a_i\\).\nThe expected relative abundance is itself a deterministic functional of the prevalence: \\[\na_i = b \\int \\frac{P(t)}{N} \\rho_i(t) dt.\n\\] Here, \\(\\rho_i\\) is a density that says how much of sample \\(i\\) was collected at different times, and \\(b\\) is a fixed conversion factor between prevalence and relative abundance. (Note \\(b = RA_p(1)\\) in the P2RA manuscript.)\n\nFrom here on, it will simplify things to specify two concrete sampling schemes. Both schemes collect evenly-spaced samples with the same depth:\n\n\\(n_i = n\\)\n\\(t_i - t_{i-1} = \\delta t\\)\n\\(0 \\leq t_0 &lt; \\delta t\\), since we don’t know how long before the first sample the pandemic began.\n\nOur two schemes differ in when the material for the samples is collected. They are:\n\nContinuous sampling: \\(\\rho_i(t) = \\frac{1}{\\delta t}\\) for \\(t \\in [t_{i-1}, t_i)\\).\nGrab sampling: \\(\\rho_i(t) = \\delta(t - t_i)\\), i.e., the whole sample is collected at \\(t_i\\). (Here, \\(\\delta(t)\\) is the Dirac delta function.)\n\nIn the next notebook, we consider a third, intermediate scheme: windowed composite sampling. In this scheme, material is collected continuously for a fixed window, usually 24 hours. (In practice, it is not sampled continuously but at regular intervals over the window.) Derivation of this case is a straightforward extension of the cases considered here, using \\(\\rho_i(t) = w^{-1}\\) for \\(t \\in [t_i - w, t_i)\\) with window length \\(w\\). For short windows (\\(w \\to 0\\)), this case approaches grab sampling. For \\(w \\to \\delta t\\) it approaches composite sampling.\n\n\nContinuous sampling\nFirst, we use the assumptions in the previous section to calculate the cumulative reads by sample \\(i\\): \\[\n\\begin{align}\nK_i & = \\sum_{j=0}^{i} n a_i \\\\\n    & = n \\sum_{j=0}^{i} b \\int_{t_{j-1}}^{t_j} \\frac{P(t)}{N} \\frac{dt}{\\delta t} \\\\\n    & = \\frac{n b}{\\delta t} \\int_{0}^{t_i} \\frac{P(t)}{N} dt.\n\\end{align}\n\\]\nAt this point, we could make the substitution \\(I(t) = (r + \\beta) P(t)\\) from our exponential model. This would give us: \\[\n\\begin{align}\nK_i & = \\frac{nb}{\\delta t} (r + \\beta) \\int_{0}^{t_i} \\frac{I(t)}{N} dt \\\\\n    & = \\frac{n b (r + \\beta)}{\\delta t} c(t_i),\n\\end{align}\n\\] which is the result from the simple calculation in the P2RA manuscript, using the conversion factor \\(r + \\beta\\) to convert between \\(RA_p\\) and \\(RA_i\\), which is only valid in the exponential growth case.\nInstead, we’ll proceed in a more general way, which will extend to the grab sampling model and account for the discrete nature of our sampling.\nUsing our exponential growth model for \\(P\\), we have: \\[\n\\begin{align}\nK_i & = \\frac{nb}{\\delta t}\n\\end{align}\n\\]\nOne somewhat subtle complication comes from the fact that we don’t know how the start of the epidemic lines up with our sampling schedule. For example, if we sample weekly, the detection time will depend on which day of the week exponential growth effectively starts (\\(t=0\\) in the model). When delay between the start of the epidemic and the first sample (\\(t_0\\)) is larger, we may eventually detect the virus one sample earlier than when the delay is smaller. In the figure below, the three colored series show \\(K_i\\) for three series of samples that differ only by the relative timing of their sample collection (e.g., all samples are collected weekly, but one series collects on Mondays, another on Tuesdays, etc.) Note that the red figure, whose first sample was taken latest relative to the start of the pandemic, crosses the threshold (solid black line) one sample earlier than the others.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt_max = 8\nr = 1.0\nk_hat = 100\ndt = 1.0\ntime = np.arange(0, t_max, 0.1)\nplt.plot(time, np.exp(r * time), \"k:\")\nfor t_0 in [0.25, 0.5, 0.75]:\n    t = np.arange(t_0, t_max, dt)\n    k = np.exp(r * t)\n    plt.plot(t, k, \"o\", label=t_0)\nplt.hlines(k_hat, 0, t_max, \"k\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cumulative reads $K$\")\nplt.xlabel(\"Time\")\nplt.title(\"Effect of sample timing\")\nplt.legend(title=\"$t_0$\")\n\n\n\n\n\n\n\n\n\nWe account for this effect by splitting \\(\\hat{t}\\) into two components:\n\nThe earliest time possible to detect, \\(t^{\\star}\\), found by allowing \\(K\\) to vary continously in time (dotted line above)\nThe residual time waiting for the next sample to be collected. This waiting time depends on the arbitrary start time of the epidemic within our sampling cycle so we have reason to expect it to take any particular value. Thus, we average it over a uniform distribution.\n\nFirst, we will find an implicit equation for \\(t^{\\star}\\): \\[\n\\begin{align}\n\\hat{K} & = \\frac{nb}{\\delta t} \\int_0^{t^{\\star}} \\frac{P(t)}{N} dt \\\\\n        & = \\frac{nb}{\\delta t} \\left(\\frac{e^{r t^{\\star}}}{rN} + \\mathcal{O}(N^{-1})\\right)\\\\\n\\end{align}\n\\]\nNext we average our target cumulative incidence over the sample waiting time: \\[\n\\begin{align}\n\\hat{c} & = \\int_{t^{\\star}}^{t^{\\star} + \\delta t} c(\\hat{t} + t_d) \\frac{d \\hat{t}}{\\delta t} \\\\\n        & = \\int_{t^{\\star}}^{t^{\\star} + \\delta t}\n                \\frac{r + \\beta}{r} \\frac{e^{r(\\hat{t} + t_d)}}{N} \\frac{d \\hat{t}}{\\delta t}\n                + \\mathcal{O}(N^{-1}) \\\\\n        & = (r + \\beta)\n            \\left(\\frac{e^{r t^{\\star}}}{r N} \\right)\n            \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n            e^{r t_d}\n            + \\mathcal{O}(N^{-1}) \\\\\n\\end{align}\n\\]\nThis result has three components:\n\nThe first two terms are the cumulative incidence at \\(t^{\\star}\\), the earliest possible detection time.\nThe third term in parentheses is the cost incurred from having widely spaced samples. As \\(r \\delta t \\to 0\\), this cost goes to zero. When \\(r \\delta t\\) is large, this cost grows exponentially.\nThe final term is the multiplier from the delay between sample collection and analysis.\n\nFinally, we notice that the second term in parentheses appears in our implicit equation for \\(t^{\\star}\\) above. Substituting and rearranging gives the sequencing effort (reads per unit time, \\(n / \\delta t\\)) required to detect by cumulative incidence \\(\\hat{c}\\): \\[\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n    e^{r t_d}\n    + \\mathcal{O}(N^{-1})\n\\]\nSome observations:\n\nFaster-growing and faster-recovering viruses require more sequencing because the current prevalence is a smaller fraction of the cumulative incidence.\nHigher detection thresholds, lower P2RA factors (\\(b\\)), and lower target cumulative incidence, require more sequencing.\nThere is a cost associated with longer sampling intervals and delays between collection and analysis. The latter grows faster than the former.\n\n\n\nGrab sampling\nNow we turn to grab sampling at the collection times \\(\\{t_0, t_1, \\ldots\\}\\). The analysis is the same as for continuous sampling, except that we will have a different implicit equation for \\(t^{\\star}\\).\nWith grab sampling, \\(\\rho_i(t) = \\delta(t - t_i)\\), we have cumulative counts: \\[\n\\begin{align}\nK_i & = n \\sum_{j=0}^{i} b \\frac{P(t_i)}{N} \\\\\n    & = \\frac{n b}{N} \\sum_{j=0}^{i} e^{r (t_0 + j \\delta t)} \\\\\n    & = \\frac{n b}{N} e^{r t_0} \\frac{e^{r(i+1)\\delta t} - 1}{e^{r\\delta t} - 1} \\\\\n    & = \\frac{n b}{\\delta t} \\left(\\frac{e^{r t_i}}{rN}\\right)\n        \\left(\\frac{r \\delta t \\, e^{r \\delta t}}{e^{r \\delta t} - 1}\\right)\n        + \\mathcal{O}(N^{-1}). \\\\\n\\end{align}\n\\] Making the continuous time substitution as before, gives \\[\n\\hat{K} = \\frac{n b}{\\delta t} \\left(\\frac{e^{r t^{\\star}}}{rN}\\right)\n            \\left(\\frac{r \\delta t \\, e^{r \\delta t}}{e^{r \\delta t} - 1}\\right)\n            + \\mathcal{O}(N^{-1}).\n\\] The first two terms are identical to the continuous sampling case. The third term is the effect of grab sampling: because the sample is collected entirely at the end of the interval, the prevalence is higher than the average over the interval and you get more reads.\nUsing this equation for \\(t^{\\star}\\), we find that the required sequencing effort for grab sampling is: \\[\n\\hat{c} = (r + \\beta)\n            \\left(\\frac{e^{r t^{\\star}}}{r N} \\right)\n            \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n            e^{r t_d}\n\\] \\[\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac\n        {e^{-r\\delta t} {\\left(e^{r \\delta t} - 1\\right)}^2}\n        {{\\left(r \\delta t\\right)}^2}\n        \\right)\n    e^{r t_d}\n    + \\mathcal{O}(N^{-1})\n\\] This is similar to the continous sampling case but with weaker dependence on the sampling interval:\n\n\nCode\nt = np.arange(0.01, 2, 0.01)\nplt.plot(t, (np.exp(t) - 1) / t, label=r\"Continuous sampling\")\nplt.plot(t, np.exp(-t) * ((np.exp(t) - 1) / t) ** 2, label=r\"Grab sampling\")\nplt.legend()\nplt.title(\"Effect of wider sampling intervals\")\nplt.xlabel(\"Sampling interval, $r \\delta t$\")\nplt.ylabel(\"Multiplicative effect on required sequencing depth\")\n\n\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8448/798707106.py:6: SyntaxWarning: invalid escape sequence '\\d'\n  plt.xlabel(\"Sampling interval, $r \\delta t$\")\n\n\n\n\n\n\n\n\n\nWe can also see the difference between the cases by comparing the Taylor series of the sampling interval delay term: \\[\\begin{align}\nf_{\\text{cont.}}(r \\delta t) & \\equiv \\frac{e^{r \\delta t} - 1}{r \\delta t} = 1 + \\frac{r \\delta t}{2} + \\frac{{(r \\delta t)}^2}{6} \\\\\nf_{\\text{grab}}(r \\delta t) & \\equiv \\frac{e^{- r \\delta t}{(e^{r \\delta t} - 1)}^2}{r \\delta t} = 1 + \\frac{{(r \\delta t)}^2}{12} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2024-02-02_CostEstimateMVP/index.html#numerical-example",
    "href": "posts/2024-02-02_CostEstimateMVP/index.html#numerical-example",
    "title": "NAO Cost Estimate MVP",
    "section": "Numerical example",
    "text": "Numerical example\nA function that calculates the depth required per unit time to detect by a given cumulative incidence:\n\ndef depth_required(\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_interval: float,\n    sampling_scheme: str,\n    delay: float,\n) -&gt; float:\n    leading_term = (\n        (growth_rate + recovery_rate)\n        * read_threshold\n        / (p2ra_factor * cumulative_incidence_target)\n    )\n    x = growth_rate * sampling_interval\n    if sampling_scheme == \"continuous\":\n        sampling_term = (np.exp(x) - 1) / x\n    elif sampling_scheme == \"grab\":\n        sampling_term = np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n    else:\n        raise ValueError(\"sampling_scheme must be continuous or grab\")\n    delay_term = np.exp(growth_rate * delay)\n    return leading_term * sampling_term * delay_term\n\nWe’ll measure time in days. Let’s assume:\n\nA virus with a doubling time of 1 week and recovery timescale of two weeks\nA read threshold of 100 viral reads\nAn \\(RA_i(1%)\\) of 1e-7 (approximate median SARS-CoV2 in Rothman)\nA cumulative incidence target of 1%\nA delay of one week for sample processing and sequencing\nVary the sampling scheme and sampling interval (from daily to monthly)\n\n\nr = np.log(2) / 7\nbeta = 1 / 14\nk_hat = 100\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\nc_hat = 0.01\ndelta_t = np.arange(1.0, 30, 1)\nt_d = 7.0\n\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nplt.plot(delta_t, 30 * n_cont, label=\"continuous\")\nplt.plot(delta_t, 30 * n_grab, label=\"grab\")\nplt.ylim([0, 5.1e10])\nplt.legend(title=\"Sampling scheme\")\nplt.ylabel(\"Reads required per month\")\nplt.xlabel(\"Sampling interval (days)\")"
  },
  {
    "objectID": "posts/2024-02-02_CostEstimateMVP/index.html#next-steps",
    "href": "posts/2024-02-02_CostEstimateMVP/index.html#next-steps",
    "title": "NAO Cost Estimate MVP",
    "section": "Next steps",
    "text": "Next steps\n\nVary the other parameters, make plots of interest?\nConsider per-sample and per-read costs. Find the optimal sampling interval for each scheme.\nRelax deterministic assumptions:\n\nPoisson read count noise\n“Excess” read count noise\nRandomness in the spread of the virus\n\nConsider multiple sites\n\nAccounting for global spread\nDetermining how to spread monitoring effort across locations"
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html",
    "title": "NAO Cost Estimate – Summary",
    "section": "",
    "text": "The goal of this project was to build a model that allows us to:\n\nEstimate the cost of sampling and sequencing required to run an effective NAO.\nCalculate the sequencing depth necessary to detect a virus by the time it reaches a target cumulative incidence.\nUnderstand which parameters are most important to understand and/or optimize to determine the viability of an NAO.\n\nWe had previously done a very simple version of this for the P2RA project. Here, we wanted to formalize the approach and include more details.\nPrevious documents:\n\nNAO Cost Estimate Outline\nSimple detection cost for P2RA\nNAO Cost Estimate MVP outline and assumptions\nNAO Cost Estimate MVP\nNAO Cost Estimate – Optimizing the sampling interval\nNAO Cost Estimate – Adding noise"
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#epidemic",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#epidemic",
    "title": "NAO Cost Estimate – Summary",
    "section": "Epidemic",
    "text": "Epidemic\nThe prevalence of the virus grows exponentially and deterministically in a single population. The fraction of people currently infectious and shedding is equal and given by: \\[\np(t) = \\frac{1}{N} e^{r t},\n\\] where \\(N\\) is the population size and \\(r\\) is the growth rate.\nThe cumulative incidence (as a fraction of the population) in this model is: \\[\nc(t) \\approx \\frac{r + \\beta}{r} p(t),\n\\] where \\(\\beta\\) is the rate at which infected people recover. Note that both prevalence and cumulative incidence grow exponentially, which is convenient for many of our calculations."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#data-collection",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#data-collection",
    "title": "NAO Cost Estimate – Summary",
    "section": "Data collection",
    "text": "Data collection\nWe collect samples from a single sampling site at regular intervals, spaced \\(\\delta t\\) apart. The material for the sample is collected uniformly over a window of length \\(w\\). (When \\(w \\to 0\\), we have a single grab sample per collection, when \\(w \\to \\delta t\\) we have continuous sampling.) Each sample is sequenced to a total depth of \\(n\\) reads.\nWe also consider the a delay of \\(t_d\\) between the collection of the sample and the data processing. This delay accounts for sample transport, sample prep, sequencing, and data analysis."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#read-counts",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#read-counts",
    "title": "NAO Cost Estimate – Summary",
    "section": "Read counts",
    "text": "Read counts\nWe considered three different models of the number of reads in each sample from the epidemic virus:\n\nA deterministic model where the number of reads in a sample at time t is \\[\nk = \\mu = n b \\int_{t-w}^{t} p(t) \\frac{dt}{w},\n\\] where \\(b\\) is the P2RA factor that converts between prevalence and relative abundance.\nA stochastic model that accounts for Poisson counting noise and variation in the latent relative abundance. In this model, the number of reads is a random variable drawn from a Poisson-gamma mixture with mean \\(\\mu\\) (as in 1.) and inverse overdispersion parameter \\(\\phi\\). Large \\(\\phi\\) means that the relative abundance is well-predicted by our deterministic model, whereas small \\(\\phi\\) means that there is a lot of excess variation beyond what comes automatically from having a finite read depth.\nA stochastic model where we sequence a pooled sample of \\(n_p\\) individuals. This allows us to consider the effect of sampling a small number of, e.g., nasal swabs rather than wastewater.\n\nSee NAO Cost Estimate – Adding noise for stochastic models."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#detection",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#detection",
    "title": "NAO Cost Estimate – Summary",
    "section": "Detection",
    "text": "Detection\nWe model detection based on the cumulative number of viral reads over all samples. When this number reaches a threshold value \\(\\hat{K}\\), the virus is detected."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#costs",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#costs",
    "title": "NAO Cost Estimate – Summary",
    "section": "Costs",
    "text": "Costs\nWe considered two components of cost:\n\nThe per-read cost of sequencing \\(d_r\\)\nThe per-sample cost of collection and processing \\(d_s\\)\n\nSee NAO Cost Estimate – Optimizing the sampling interval for details."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#sequencing-effort-required-in-a-deterministic-model",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#sequencing-effort-required-in-a-deterministic-model",
    "title": "NAO Cost Estimate – Summary",
    "section": "Sequencing effort required in a deterministic model",
    "text": "Sequencing effort required in a deterministic model\nIn NAO Cost Estimate MVP, we found the sampled depth per unit time required to detect a virus by the time it reaches cumulative incidence \\(\\hat{c}\\) to be: \\[\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac\n        {e^{-r\\delta t} {\\left(e^{r \\delta t} - 1\\right)}^2}\n        {{\\left(r \\delta t\\right)}^2}\n        \\right)\n    e^{r t_d}.\n\\] This result is for grab sampling, which in our model is a good approximation for windowed-composite sampling when \\(r w \\ll 1\\).\nThe first two terms on the right-hand side are equivalent to the result from the P2RA model using the conversion between prevalence and incidence implied by our exponential growth model.\nThe third term in parentheses is an adustment factor for collecting samples at \\(\\delta t\\) intervals. It includes two factors:\n\nthe delay between when the virus is theoretically detectable and the next sample taken, and\nthe benefit of taking a grab sample late in the sampling interval when the prevalence is higher.\n\nThis term has Taylor expansion \\(1 + \\frac{{(r \\delta t)}^2}{12} + \\mathcal{O}{(r\\delta t)}^3\\).\nThe final term is the cost of the \\(t_d\\) delay between sampling and data processing."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#optimal-sampling-interval",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#optimal-sampling-interval",
    "title": "NAO Cost Estimate – Summary",
    "section": "Optimal sampling interval",
    "text": "Optimal sampling interval\nIn NAO Cost Estimate – Optimizing the sampling interval, we found the sampling interval \\(\\delta t\\) that minimized the total cost. Longer \\(\\delta t\\) between samples saves money on sample processing, but requires more depth to make up for the delay of waiting for the next sample after the virus becomes detectable. We found that the optimal \\(\\delta t\\) satisfies (again for grab sampling):\n\\[\nr \\delta t \\approx {\\left(\n    6 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{k}}\n    \\left( \\frac{r}{r + \\beta} \\right)\n    e^{- r t_d}\n    \\right)}^{1/3}.\n\\]\nWhen sampling optimally, the per-sample sequencing cost (\\(n d_r\\)) should be a multiple of the sample costs(\\(d_s\\)): \\[\nn d_r \\approx \\frac{6}{{\\left(r \\delta t\\right)}^2} d_s\n\\]"
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#additional-sequencing-required-to-ensure-a-high-probability-of-detection",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#additional-sequencing-required-to-ensure-a-high-probability-of-detection",
    "title": "NAO Cost Estimate – Summary",
    "section": "Additional sequencing required to ensure a high probability of detection",
    "text": "Additional sequencing required to ensure a high probability of detection\nIn NAO Cost Estimate – Adding noise, we change our detection criterion from requiring the expected number of reads to reach the threshold \\(\\hat{K}\\) to requiring that the number of reads reach \\(\\hat{K}\\) with high probability, \\(p\\). We ask how much higher the cumulative incidence has to be to meet the second criterion than the first.\nWe find that a key parameter is \\(\\nu = \\frac{\\phi}{r \\delta t}\\), which measures the departure of the read count distribution from Poisson. When \\(\\mu / \\nu \\ll 1\\), the Poisson noise dominates and our detection criterion is: \\[\n\\hat{K} \\approx \\mu + \\mu^{1/2} \\Phi^{-1}(1 - p)\n\\] where \\(\\Phi^{-1}\\) is the inverse CDF of a standard Gaussian distribution. Solving this equation for \\(\\mu\\) gives the corresponding number of copies in the deterministic model required to detect with probability \\(p\\).\nWhen \\(\\mu / \\nu \\gg 1\\), the Poisson noise is small compared to the variation in the latent relative abundance. Here the detection criterion is: \\[\n\\hat{K} \\approx \\mu \\left(1 + \\frac{1}{{(2\\nu)}^{1/2}} w_p(\\nu) \\right)\n\\] where \\(w_{1-p}(\\nu) &lt; 0\\) is a function that measures the departure of the distribution from Gaussian at quantile \\(1-p\\). The term in parentheses is thus less than one and measures the ratio of the detection threshold to the deterministic approximation at detection.\nNumerical exploration of these regimes suggests that we expect to need 1.5–3 times more sequencing than the deterministic model predicts to detect with 95% probability by the target cumulative incidence."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#small-pool-noise",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#small-pool-noise",
    "title": "NAO Cost Estimate – Summary",
    "section": "Small pool noise",
    "text": "Small pool noise\nIn the Appendix to the noise post, we showed that the effect of pooling a small number of samples is controlled by \\(a\\), the average number of viral reads each infected person contributes to the sample. With fixed sampling depth, \\(a\\) is inversely proportional to the pool size \\(n_p\\). We found that if the detection threshold is one read \\(\\hat{K} = 1\\), sequencing depth required to ensure a given probability of detection increases in proportion to \\[\n\\frac{a}{1 - e^{-a}}.\n\\] We expect a similar result to hold for higher detection thresholds."
  },
  {
    "objectID": "posts/2024-03-25_CostEstimateSummary/index.html#potential-extensions",
    "href": "posts/2024-03-25_CostEstimateSummary/index.html#potential-extensions",
    "title": "NAO Cost Estimate – Summary",
    "section": "Potential extensions",
    "text": "Potential extensions\n\nWe could turn this analysis into a “plausibility map”: Given a system design (budget or \\(n\\), \\(\\delta t\\), \\(t_d\\), etc.), what ranges of growth rates and P2RA factors could we detect reliably by a target cumulative incidence?\nWe could extend the model to consider multiple sampling sites.\nThe current epidemic model is completely deterministic. It would be good to check whether adding randomness changes our conclusions. (I suspect it won’t in a single-population model, but may matter for multiple sampling sites.)\nWe could consider a more sophisticated detection model than just cumulative reads. For example we could analyze a toy model of EGD.\nWe could explore the noise distribution of real data and try to measure \\(\\phi\\) and whether the latent noise is mostly independent or correlated between samples."
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "",
    "text": "See previous notebook. The goal of this notebook is to use our simple, deterministic cost estimate to answer the question:\n\nHow often should we process and sequence samples?\n\nWe want to understand the tradeoff between:\n\ncatching the virus earlier by sampling more frequently, and\nsaving on processing costs by sampling less frequently.\n\nTo this end, we posit a two-component cost model:\n\nPer-read sequencing costs, and\nPer-sample processing costs\n\nand find the optimal sampling interval \\(\\delta t\\) that minimizes total costs, while sequencing to sufficient depth per sample \\(n\\) to detect a virus by cumulative incidence \\(\\hat{c}\\)."
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html#background",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html#background",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "",
    "text": "See previous notebook. The goal of this notebook is to use our simple, deterministic cost estimate to answer the question:\n\nHow often should we process and sequence samples?\n\nWe want to understand the tradeoff between:\n\ncatching the virus earlier by sampling more frequently, and\nsaving on processing costs by sampling less frequently.\n\nTo this end, we posit a two-component cost model:\n\nPer-read sequencing costs, and\nPer-sample processing costs\n\nand find the optimal sampling interval \\(\\delta t\\) that minimizes total costs, while sequencing to sufficient depth per sample \\(n\\) to detect a virus by cumulative incidence \\(\\hat{c}\\)."
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html#a-two-component-cost-model",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html#a-two-component-cost-model",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "A two-component cost model",
    "text": "A two-component cost model\nConsider the cost averaged over a long time interval \\(T\\) in which we will take many samples. If we collect and process samples every \\(\\delta t\\) days, we will take \\(T / \\delta t\\) samples in this interval. If we sample \\(n\\) reads per sample, our total sequencing depth is \\(n \\frac{\\delta t}{T}\\) reads. Assume that our costs can be divided into a per-sample cost \\(d_s\\) (including costs of collection, transportation, and processing for sequencing) and a per-read cost \\(d_r\\) of sequencing. (Note: the \\(d\\) is sort of awkward because we’ve already used \\(c\\) for “cumulative incidence”. You can think of it as standing for “dollars”.)\nWe will seek to minimize the total cost of detection: \\[\nd_{\\text{tot}} = d_s \\frac{T}{\\delta t} + d_r \\frac{nT}{\\delta t}.\n\\] Equivalently, we can divide by the arbitrary time-interval \\(T\\) to get the total rate of spending: \\[\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + d_r \\frac{n}{\\delta t}.\n\\]\nIn our previous post, we found that the read depth per time required to detect a virus by the time it reaches cumulative incidence \\(\\hat{c}\\) is:\n\\[\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} f(r \\delta t)\n\\]\nwhere the function \\(f\\) depends on the sampling scheme. Substituting this into the rate of spending, we have: \\[\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} d_r f(r \\delta t).\n\\]\nIn the next section, we will find the value of \\(\\delta t\\) that minimizes the rate of spending.\n\nLimitations of the two-component model\n\nWe assume that we process each sample as it comes in. In practice, we could stockpile a set of \\(m\\) samples and process them simultaneously. This would require splitting out the cost of sampling from the cost of sample prep.\nWe do not consider the fact that sequencing (and presumably to some extent sample prep) unit costs decrease with greater depth. (I.e., it’s cheaper per-read to do bigger runs.)\nWe neglect the “batch” effects of sequencing. Typically you buy sequencing in units of “lanes” rather than asking for an arbitrary number of reads. This will introduce threshold effects, where we want to batach our samples to use lanes efficiently.\nWe do not account for fixed costs that accumulate per unit time regardless of our sampling and sequencing protocols. These do not affect the optimization here, but they do add to the total cost of the system."
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html#optimizing-the-sampling-interval",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html#optimizing-the-sampling-interval",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "Optimizing the sampling interval",
    "text": "Optimizing the sampling interval\nTo find the optimal \\(\\delta t\\), we look for a zero of the derivative of spending rate:\n\\[\n\\begin{align}\n\\frac{d}{d \\delta t} \\frac{d_{\\text{tot}}}{T} & = - \\frac{d_s}{{\\delta t}^2} + (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} d_r r f'(r\\delta t).\n\\end{align}\n\\]\nSetting the right-hand side equal to zero and rearranging gives:\n\\[\n{(r \\delta t)}^2 f'(r \\delta t) = \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}\n\\]\nTo get any farther, we need to specify \\(f\\) and therefore a sampling scheme. [Note: If we give some general properties of \\(f\\), we can say some things here that are general to the sampling scheme]\n\nGrab sampling\nWe first consider grab sampling, where the entire sample is collected at the sampling time. In that case, we have: \\[\n\\begin{align}\nf(x) & = \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & = 1 + \\frac{x^2}{12} + \\mathcal{O}(x^3).\n\\end{align}\n\\] We are particularly interested in the small-\\(x\\) regime: The depth required becomes exponentially large when \\(r \\delta t \\gg 1\\), so it is likely that the optimal interval satisfies \\(r \\delta t \\lesssim 1\\). We can check this for self-consistency in any specific numerical examples.\nThis gives us the derivative: \\[\nf'(x) \\approx \\frac{x}{6}.\n\\]\nUsing this in our optimization equation yields: \\[\n{(r \\delta t)}^3 \\approx 6 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n\\]\n\n\nContinuous sampling\nIn the case of continuous sampling, where the sample taken at time \\(t\\) is a composite sample uniformly collected over the interval \\([t - \\delta t, t)\\), we have: \\[\n\\begin{align}\nf(x) & = \\frac{e^x - 1}{x} \\\\\n     & = 1 + \\frac{x}{2} + \\mathcal{O}(x^2) \\\\\nf'(x) & \\approx \\frac{1}{2}\n\\end{align}\n\\] for small \\(x\\). Note the difference in functional form from the grab sample case.\nSubstituting this into the optimization equation yields: \\[\n{(r \\delta t)}^2 \\approx 2 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n\\]\n\n\nWindowed composite sampling\nAn intermediate (and more realistic) model of sampling is windowed composite sampling. In this scheme, the sample at time \\(t\\) is a composite sample taken over a window of width \\(w\\) (e.g., 24hours) from \\(t - w\\) to \\(t\\). Notably, when the sampling interval (\\(\\delta t\\)) increases, the length of the window does not. In this case we have:\n\\[\n\\begin{align}\nf(x) & = \\frac{rw}{1 - e^{-rw}} \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & \\approx \\left(1 + \\frac{rw}{2}\\right) \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & \\approx \\left(1 + \\frac{rw}{2}\\right) + \\left(1 + \\frac{rw}{2}\\right) \\left(\\frac{x^2}{12}\\right) \\\\\nf'(x) & \\approx \\left(1 + \\frac{rw}{2}\\right) \\left(\\frac{x}{6}\\right) \\\\\n\\end{align}\n\\] for small \\(rw\\) and \\(x\\). Note that as \\(rw \\to 0\\), we recover grab sampling.\nSince we’re keeping only the leading term in \\(x = r \\delta t\\), and \\(w \\leq \\delta t\\), for consistency we should also drop the \\(\\frac{rw}{2}\\) (or keep more terms of the expansion). Thus, we’ll treat windowed composite sampling for small windows as equivalent to grab sampling. The key reason for this is that changing the sampling interval does not change the window. Note that for \\(\\delta t \\approx w\\), i.e. \\(x \\approx rw\\), \\(f(x) \\approx 1 + \\frac{rw}{2}\\), just as with continuous sampling, but \\(f'(x)\\) still behaves like grab sampling.\n\n\nGeneral properties\nIn general, we have: \\[\nr \\delta t \\approx {\\left( a\\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d} \\right)}^{1 / \\gamma},\n\\] where \\(a\\) and \\(\\gamma\\) are positive constants that depend on the sampling scheme. We can observe some general features:\n\nFaster-growing viruses (higher \\(r\\)) decreases the optimal sampling interval.\nIncreasing the cost per sample \\(d_s\\) increases the optimal sampling interval.\nIncreasing the cost per read \\(d_r\\) decreases the optimal sampling interval.\nIncreasing the P2RA factor \\(b\\) or the target cumulative incidence \\(c\\) increases the optimal sampling interval.\nIncreasing the detection threshold \\(\\hat{K}\\) decreases the optimal sampling interval.\nIncreasing the delay between sampling and detection \\(t_d\\) decreases the optimal sampling interval.\n\nOne general trend is: the more optimistic we are about our method (higher \\(b\\), smaller \\(\\hat{K}\\), shorter \\(t_d\\)), the longer we can wait between samples.\nWe can also substitute our equation for \\(n / \\delta t\\) into this equation, use \\(f(\\delta t) \\approx 1\\) and rearrange to get: \\[\nn d_r \\approx \\frac{a}{{(r \\delta t)}^{\\gamma - 1}} d_s.\n\\] The left-hand side of this equation is the cost spent on sequencing per sample. For continuous sampling, \\(\\gamma = 2\\) and for grab sampling and windowed composite, \\(\\gamma = 3\\). Since \\(r \\delta t \\ll 1\\), this tells us that we typically should spend more money on sequencing than sample processing."
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html#a-numerical-example",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html#a-numerical-example",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "A numerical example",
    "text": "A numerical example\n\nOptimal \\(\\delta t\\)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Optional\n\n\ndef optimal_interval(\n    per_sample_cost: float,\n    per_read_cost: float,\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_scheme: str,\n    delay: float,\n    composite_window: Optional[float] = None,\n) -&gt; float:\n    constant_term = (\n        (per_sample_cost / per_read_cost)\n        * ((p2ra_factor * cumulative_incidence_target) / read_threshold)\n        * (growth_rate / (growth_rate + recovery_rate))\n        * np.exp(-growth_rate * delay)\n    )\n    if sampling_scheme == \"continuous\":\n        a = 2\n        b = 2\n    elif sampling_scheme == \"grab\":\n        a = 6\n        b = 3\n    elif sampling_scheme == \"composite\":\n        if not composite_window:\n            raise ValueError(\"For composite sampling, must provide a composite_window\")\n        a = (\n            6\n            * (1 - np.exp(-growth_rate * composite_window))\n            / (growth_rate * composite_window)\n        )\n        b = 3\n    else:\n        raise ValueError(\"sampling_scheme must be continuous or grab\")\n    return (a * constant_term) ** (1 / b) / growth_rate\n\n\nI asked the NAO in Twist for info on sequencing and sample-processing costs. Based on their answers, reasonable order-of-magnitude estimate are:\n- sample costs: $500 / sample\n- sequencing costs: $5K / billion reads\nNote that 1 billion reads cost roughly 10x the cost to prepare one sample. As discussed above, our cost model is highly simplified and the specifics of when samples are collected, transported, processed, and batched for sequencing will make this calculation much more complicated in practice.\nLet’s use these numbers plus our virus model from the last post to find the optimal sampling interval:\n\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Weekly doubling\nr = np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n# Delay from sampling to detecting of 1 week\nt_d = 7.0\n\ndelta_t_grab = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\ndelta_t_cont = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"continuous\", t_d)\ndelta_t_24hr = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1)\n\nprint(f\"Optimal sampling interval with grab sampling:\\t\\t{delta_t_grab:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_grab:.2f}\")\nprint(f\"Optimal sampling interval with continuous sampling:\\t{delta_t_cont:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_cont:.2f}\")\nprint(\n    f\"Optimal sampling interval with 24-hour composite sampling:\\t{delta_t_24hr:.2f} days\"\n)\nprint(f\"\\tr delta_t = {r*delta_t_24hr:.2f}\")\n\nOptimal sampling interval with grab sampling:       5.98 days\n    r delta_t = 0.59\nOptimal sampling interval with continuous sampling: 2.66 days\n    r delta_t = 0.26\nOptimal sampling interval with 24-hour composite sampling:  5.89 days\n    r delta_t = 0.58\n\n\nWe should check that \\(r \\delta_t\\) is small enough that our approximation for \\(f(x)\\) is accurate:\n\n\nCode\nx = np.arange(0.01, 3, 0.01)\nplt.plot(x, np.exp(-x) * ((np.exp(x) - 1) / x) ** 2, label=\"exact\")\nplt.plot(x, 1 + x**2 / 12, label=\"approx\")\nplt.ylim([0, 2])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Grab/24hr-composite sampling\")\nplt.show()\n\nplt.plot(x, (np.exp(x) - 1) / x, label=\"exact\")\nplt.plot(x, 1 + x / 2, label=\"approx\")\nplt.ylim([0, 5])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Continuous sampling\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks fine in both cases.\n\n\nCost sensitivity to \\(\\delta t\\)\nIn a real system, we won’t be able to optimize \\(\\delta t\\) exactly. Let’s see how the cost varies with the sampling interval (using the exact \\(f\\)):\n\n\nCode\ndef depth_required(\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_interval: float,\n    sampling_scheme: str,\n    delay: float,\n    composite_window: Optional[float] = None,\n) -&gt; float:\n    leading_term = (\n        (growth_rate + recovery_rate)\n        * read_threshold\n        / (p2ra_factor * cumulative_incidence_target)\n    )\n    x = growth_rate * sampling_interval\n    if sampling_scheme == \"continuous\":\n        sampling_term = (np.exp(x) - 1) / x\n    elif sampling_scheme == \"grab\":\n        sampling_term = np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n    elif sampling_scheme == \"composite\":\n        if not composite_window:\n            raise ValueError(\"For composite sampling, must provide a composite_window\")\n        rw = growth_rate * composite_window\n        sampling_term = (\n            (rw / (1 - np.exp(-rw))) * np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n        )\n    else:\n        raise ValueError(\"sampling_scheme must be continuous, grab, or composite\")\n    delay_term = np.exp(growth_rate * delay)\n    return leading_term * sampling_term * delay_term\n\n\ndef cost_per_time(\n    per_sample_cost: float,\n    per_read_cost: float,\n    sampling_interval: float,\n    sample_depth_per_time: float,\n) -&gt; float:\n    return sample_cost_per_time(per_sample_cost, sampling_interval) + seq_cost_per_time(\n        per_read_cost, sample_depth_per_time\n    )\n\n\ndef sample_cost_per_time(per_sample_cost, sampling_interval):\n    return per_sample_cost / sampling_interval\n\n\ndef seq_cost_per_time(per_read_cost, sample_depth_per_time):\n    return per_read_cost * sample_depth_per_time\n\n\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nn_24hr = depth_required(\n    r, beta, k_hat, b, c_hat, delta_t, \"composite\", t_d, composite_window=1.0\n)\ncost_cont = cost_per_time(d_s, d_r, delta_t, n_cont)\ncost_grab = cost_per_time(d_s, d_r, delta_t, n_grab)\ncost_24hr = cost_per_time(d_s, d_r, delta_t, n_24hr)\nplt.plot(delta_t, cost_cont, label=\"continuous\")\nplt.plot(delta_t, cost_grab, label=\"grab\")\nplt.plot(delta_t, cost_24hr, label=\"24hr composite\")\nplt.ylim([0, 5000])\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nFirst, note that the cost of 24hr composite sampling is quite close to grab sampling, and that when the sampling interval is 1 day, it is exactly the same as continuous sampling.\nIt looks like the cost curve is pretty flat for the grab/24hr sampling, suggesting that we could choose a range of sampling intervals without dramatically increasing the cost. For continuous sampling, the cost increases more steeply with increasing sampling interval.\nFinally, let’s break the costs down between sampling and sequencing:\n\n\nCode\nplt.plot(delta_t, cost_grab, label=\"Total\")\nplt.plot(delta_t, sample_cost_per_time(d_s, delta_t), label=\"Sampling\")\nplt.plot(delta_t, seq_cost_per_time(d_r, n_grab), label=\"Sequencing\")\nplt.legend()\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.title(\"Grab sampling\")\n\n\n\n\n\n\n\n\n\nWe can observe a few things:\n\nSequencing costs are always quite a bit higher than sampling costs.\nIncreasing the sampling interval from one day to about five generates a significant savings in sampling cost, any longer than that gives strongly diminishing returns. (This makes sense from the functional form \\(d_s / \\delta t\\).)\nThe required sequencing depth increases slowly in this range.\n\n\n\nSensitivity of optimal \\(\\delta t\\) to P2RA factor\nWe have a lot of uncertainty in the P2RA factor, even for a specific known virus with a fixed protocol. Let’s see how the optimal sampling interval varies with it. (We’ll only do this for grab sampling.)\n\n\nCode\nra_i_01 = np.logspace(-9, -6, 100)\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\n\nplt.semilogx(ra_i_01, delta_t_opt)\nplt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\nplt.ylabel(\"Optimal sampling interval, $\\delta t$\")\nplt.ylim([0, 13])\n\n\n&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\%'\n&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\%'\n&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/3336883113.py:8: SyntaxWarning: invalid escape sequence '\\%'\n  plt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/3336883113.py:9: SyntaxWarning: invalid escape sequence '\\d'\n  plt.ylabel(\"Optimal sampling interval, $\\delta t$\")\n\n\n\n\n\n\n\n\n\nAs expected, the theory predicts that with higher P2RA factors, we can get away with wider sampling intervals. Also, for this range of P2RA factors, it never recommends daily sampling.\nHowever, we can also see that the cost per day depends much more strongly on the P2RA factor than on optimizing the sampling interval:\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nfor ra_i_01 in [1e-8, 1e-7, 1e-6]:\n    b = ra_i_01 * 100 * (r + beta) * 7\n    n = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\n    cost = cost_per_time(d_s, d_r, delta_t, n)\n    plt.plot(delta_t, cost, label=f\"{ra_i_01}\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend(title=r\"$RA_i(1\\%)$\")"
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html#a-second-example-faster-growth-and-longer-delay",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html#a-second-example-faster-growth-and-longer-delay",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "A second example: Faster growth and longer delay",
    "text": "A second example: Faster growth and longer delay\nLet’s consider a more pessimistic scenario: doubling both the growth rate and the delay to detection.\n\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Twice-weekly doubling\nr = 2 * np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n# Delay from sampling to detecting of 2 weeks\nt_d = 14.0\n\ndelta_t_grab = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\ndelta_t_cont = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"continuous\", t_d)\ndelta_t_24hr = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1)\n\nprint(f\"Optimal sampling interval with grab sampling:\\t\\t{delta_t_grab:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_grab:.2f}\")\nprint(f\"Optimal sampling interval with continuous sampling:\\t{delta_t_cont:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_cont:.2f}\")\nprint(\n    f\"Optimal sampling interval with 24-hour composite sampling:\\t{delta_t_24hr:.2f} days\"\n)\nprint(f\"\\tr delta_t = {r*delta_t_24hr:.2f}\")\n\nOptimal sampling interval with grab sampling:       1.88 days\n    r delta_t = 0.37\nOptimal sampling interval with continuous sampling: 0.66 days\n    r delta_t = 0.13\nOptimal sampling interval with 24-hour composite sampling:  1.82 days\n    r delta_t = 0.36\n\n\nWe should check that \\(r \\delta_t\\) is small enough that our approximation for \\(f(x)\\) is accurate:\n\n\nCode\nx = np.arange(0.01, 3, 0.01)\nplt.plot(x, np.exp(-x) * ((np.exp(x) - 1) / x) ** 2, label=\"exact\")\nplt.plot(x, 1 + x**2 / 12, label=\"approx\")\nplt.ylim([0, 2])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Grab sampling\")\nplt.show()\n\nplt.plot(x, (np.exp(x) - 1) / x, label=\"exact\")\nplt.plot(x, 1 + x / 2, label=\"approx\")\nplt.ylim([0, 5])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Continuous sampling\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks fine in both cases.\n\nCost sensitivity to \\(\\delta t\\)\nIn a real system, we won’t be able to optimize \\(\\delta t\\) exactly. Let’s see how the cost varies with the sampling interval:\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nn_24hr = depth_required(\n    r, beta, k_hat, b, c_hat, delta_t, \"composite\", t_d, composite_window=1.0\n)\ncost_cont = cost_per_time(d_s, d_r, delta_t, n_cont)\ncost_grab = cost_per_time(d_s, d_r, delta_t, n_grab)\ncost_24hr = cost_per_time(d_s, d_r, delta_t, n_24hr)\nplt.plot(delta_t, cost_cont, label=\"continuous\")\nplt.plot(delta_t, cost_grab, label=\"grab\")\nplt.plot(delta_t, cost_24hr, label=\"24hr composite\")\nplt.ylim([0, 100000])\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nIt looks like the cost curve is pretty flat for the grab sampling, suggesting that we could choose a range of sampling intervals without dramatically increasing the cost. For continuous sampling, the cost increases more steeply with increasing sampling interval.\nFinally, let’s break the costs down between sampling and sequencing:\n\n\nCode\nplt.plot(delta_t, cost_grab, label=\"Total\")\nplt.plot(delta_t, sample_cost_per_time(d_s, delta_t), label=\"Sampling\")\nplt.plot(delta_t, seq_cost_per_time(d_r, n_grab), label=\"Sequencing\")\nplt.legend()\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.title(\"Grab sampling\")\n\n\n\n\n\n\n\n\n\nIn this faster growth + more delay example, sequencing costs completely dwarf sampling costs.\n\n\nSensitivity of optimal \\(\\delta t\\) to P2RA factor\n\n\nCode\nra_i_01 = np.logspace(-9, -6, 100)\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\n\nplt.semilogx(ra_i_01, delta_t_opt)\nplt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\nplt.ylabel(\"Optimal sampling interval, $\\delta t$\")\nplt.ylim([0, 5])\n\n\n&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\%'\n&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\%'\n&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/223304510.py:8: SyntaxWarning: invalid escape sequence '\\%'\n  plt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\n/var/folders/dv/_dgh3jnn7kn32ndcd117mg5m0000gn/T/ipykernel_8637/223304510.py:9: SyntaxWarning: invalid escape sequence '\\d'\n  plt.ylabel(\"Optimal sampling interval, $\\delta t$\")\n\n\n\n\n\n\n\n\n\nIn this case, daily sampling is sometimes favored when the P2RA factor is small enough.\nHowever, we can also see that the cost per day depends much more strongly on the P2RA factor than on optimizing the sampling interval:\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nfor ra_i_01 in [1e-8, 1e-7, 1e-6]:\n    b = ra_i_01 * 100 * (r + beta) * 7\n    n = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\n    cost = cost_per_time(d_s, d_r, delta_t, n)\n    plt.plot(delta_t, cost, label=f\"{ra_i_01}\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend(title=r\"$RA_i(1\\%)$\")"
  },
  {
    "objectID": "posts/2024-02-08_OptimalSamplingInterval/index.html#cost-sensitivity-to-the-latency-t_d",
    "href": "posts/2024-02-08_OptimalSamplingInterval/index.html#cost-sensitivity-to-the-latency-t_d",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "Cost sensitivity to the latency, \\(t_d\\)",
    "text": "Cost sensitivity to the latency, \\(t_d\\)\nAs a final application, let’s calculate what the optimal cost would be as a function of delay/latency time \\(t_d\\). We’ll use 24-hr composite sampling. And for some realism, we’ll round the optimal sampling interval to the nearest day.\n\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Bi-weekly doubling\nr = 2 * np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n\n\n\nCode\nt_d = np.arange(1.0, 22.0, 1.0)\ndelta_t_opt = optimal_interval(\n    d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1.0\n)\ndelta_t_round = np.round(delta_t_opt)\nn = depth_required(r, beta, k_hat, b, c_hat, delta_t_round, \"composite\", t_d, 1.0)\ncost = cost_per_time(d_s, d_r, delta_t_round, n)\n\nplt.plot(t_d, delta_t_round, \"o\")\nplt.ylim([0, 5])\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(r\"Optimal sampling interval $\\delta t$ (days)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nShorter latency means that we can sample less often.\n\n\nCode\nplt.plot(t_d, n, \"o\")\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(\"Depth per day (reads)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLonger latency means that we have to sequence exponentially more reads per day. This leads to exponentially higher costs:\n\n\nCode\nplt.plot(t_d, cost, \"o\")\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(\"Cost per day (dollars)\")\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dan's NAO notebook",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n2025-07-03\n\n\nSimulating the sensitivity of swabs and wastewater\n\n\n\n\n2024-03-25\n\n\nNAO Cost Estimate – Summary\n\n\n\n\n2024-03-04\n\n\nNAO Cost Estimate MVP – Adding noise\n\n\n\n\n2024-02-13\n\n\nNAO Cost Estimate MVP – Optimizing the sampling interval\n\n\n\n\n2024-02-02\n\n\nNAO Cost Estimate MVP\n\n\n\n\n\nNo matching items"
  }
]